Technology and Cyberspace

Introduction

Of all the technologies that shape contemporary life, computer technology is arguably the most powerful and pervasive. We commonly speak of “the computer revolution,” the rise of a global “computer culture,” and “the age of [computer-processed] information.” Not everyone, of course, is happy about this; but it is hard to find anyone ready to assure us that the computer’s influence matters little. At the same time, however, aside from those who are selling their own wares, it is difficult to find anyone who rejoices in the coming of cyberculture without at least some qualifications.

Nowhere has this question been fought out in greater detail and by more philosophers, engineers, scientists, and proponents of technoscience studies than in connection with artificial intelligence (AI) research. What sort of “mind” can “know” something? Human only? And what if, instead, we are on the verge of creating artificial minds, or robots with minds, or even cyborgs verging on virtual (or “post-”) humanity (or vice versa)? And what does it mean that we are even thinking that we should try to do this? Some are happy and upbeat about the whole prospect, some are in varying degrees skeptical or critical, and some are only skeptical or critical about the current course and prospects of AI research, while others are more deeply suspicious of the philosophical understanding of humanity, mentality, science, and technology that underlies it.

In what might usefully be read as a paired set of articles, Daniel Dennett and Hubert L. Dreyfus represent two radically different philosophical perspectives on the AI issue, each focused here on how it emerges in connection with robotics. The main question is: Can we build a “conscious robot”? Dennett, famously, takes the hard line: Yes, of course, because “we are a sort of robot ourselves.” Dreyfus, almost as famously (many AI researchers would say, notoriously), rejects this hard line: Until AI researchers and robotic engineers acquire a richer, more phenomenological understanding of embodied human life than hardliners now presuppose, results will be continue to be crude, limited, and oversold.

Dennett begins his wonderfully clear and pointed discussion by asserting that if we never build a conscious robot, it will be because it costs too much or the engineering is too challenging, not because trying to build it would contradict some natural law. After replying to a few objections, he considers MIT’s well-known Cog project (closed down in 2003 after he wrote this essay). Several of the most important features of Dennett’s outlook are prominently on display throughout his discussion. First, Dennett holds a functional or computational position regarding mentality – namely, that thought-activity is related to the brain in the same way as computational activity is to the computer. Assuming this functional identity is what enables him to assert that “we” are a kind of robot ourselves. Second, Dennett’s view represents (with some disagreements in detail) what is usually called strong AI (dubbed by John Haugeland “Good Old-Fashioned AI” or GOFAI), which holds that since human intelligence consists of nothing but the application of rules to facts (i.e., is “symbolic representation”), creating computers/robots that respond like intelligent humans boils down to writing programs for processing data that are sophisticated enough for the computers/robots to perform human tasks. Third, Dennett is a scientific naturalist, which means that he understands reality to be, and be only, what scientific investigation can find and theorize about (hence his famous critiques of religious doctrines and immaterial minds as supernaturalist fictions). Finally, given a naturalistic ontology, he is also committed to an externalist, or third-person “observational” standpoint (a very unphenomenological outlook that he provocatively calls “heterophenomenology”). Thus for Dennett, the “problem of consciousness” must be treated entirely in the same physical-scientific terms as any other phenomenon.

Regarding objections, Dennett’s general strategy is to find behind each of them the allegiance to a philosophical outlook that is not scientific naturalism. In response to the argument that consciousness cannot be understood materialistically, Dennett ridicules its dependence on the traditional sort of mind–body dualism that invariably winds up imagining that are some immaterial or non-natural substances like “selves” or mysterious “qualia” available only to private intuition or introspection. In fact, he says, every such phenomenon has eventually turned out to be either an obvious fiction or something perfectly intelligible, say, to molecular biology. To the related objection that consciousness requires an “organic” host, not a machine, Dennett replies that this raises at most a burdensome engineering problem – perhaps even too burdensome for us ever to actually build an entirely non-organic brain-like host – but this, then, is not a philosophical objection. To the objection that because consciousness is “born,” it cannot be mechanically produced, Dennett argues that this either amounts to a dogmatic sort of “origin chauvinism” or it mystifies the serious but purely practical problem of “making” a fully competent adult robot. We undoubtedly have to start, he says, with something more child-like. Finally, he dismisses as “scientifically boring” the appeal to insurmountable complexity. People also used to say this about producing mechanical heart valves, ear implants, and other prostheses. Of course, at the moment we may still have only simpler and cruder devices, but where is the justification for leaping to negative conclusions about what cannot be built in principle?

Dennett’s consideration of MIT’s humanoid robot (“Cog”) project starts with the barbed suggestion that ongoing real-world projects might “teach us something interesting” while outsiders continue to fiddle with “philosophical conundrums.” This project, he says, was inspired by the promising idea (perhaps reinforced by objections from a younger generation of robotic engineers not as hostile to phenomenology?) that some replication of human “embodiment” through the addition of physical mobility, optical sensors, and a somewhat human-like form might improve robot learning opportunities both “perceptually” and through more successful “social” interaction. Dennett admits that one problem even for Cog is that learning some behaviors might prove too difficult without hard-wiring in some organizing rules as “innate endowments.” He mentions the difficulty of face recognition, but the most notorious of these problems is one he himself discovered by extrapolating from a formal-logical dilemma with the same name, namely, “the frame problem” (mentioned by Dreyfus below, in connection with the problem of giving robots “common sense”). Basically, the difficulty involves learning, not how to perform a task, but how to select what is relevant (and to deselect the millions of irrelevant considerations) for a task, or even how to identify something as a task. (“Hammering this nail into this piece of wood” is easy; playing this game of chess is harder; but attending a birthday party, or shopping for the week’s groceries…?) Obviously, if too much is built in – if, as one says in traditional terms, too much of the robot’s “knowledge” is a priori – the machine might well “behave” better, but one can no longer say it is “replicating” a human-like consciousness. What Dennett calls the “team of [programming] overseers” would become a kind of deus ex machina – or maybe better, the “consciousness” of the machine and its programs.

As Dreyfus’ discussion makes clear, a number of very sophisticated philosophical positions now lie between GOFAI and the phenomenological standpoint he takes in his essay. For this anthology, however, the main goal is not to survey or evaluate the relative merits of these positions but to connect their debates to technology studies. Hence, it is useful to begin with the story of Dreyfus’ initial unhappiness with GOFAI, in order to highlight features of his philosophical standpoint that are relevant to material in other parts of this collection. He tells us that, as a young MIT instructor, he became suspicious of claims made by his computer science students – that they were “solving” all the classical problems of philosophy about the nature of knowledge (already being referred to as “intelligence”). When he looked into their claims, he discovered that far from replacing philosophy with real research, they were in fact, quite unreflectively and unfortunately, merely “turn[ing central elements of modern] philosophical rationalism into a research program.” In a provocative RAND Corporation report, Alchemy and Artificial Intelligence (1965), Dreyfus explains in detail how deeply flawed this philosophical understanding of human knowing actually is and why adopting it doomed MIT’s AI program in advance. Referring especially to Heidegger’s Being and Time, he argues there that, except for fairly elementary tasks, we humans process information in ways that depend upon (a) a tolerance of ambiguity, (b) an ability to separate focal and fringe awareness, and (c) a capacity to discriminate between what is essential and inessential – all features of ordinary human knowing ignored by MIT’s preference for computer models of intelligence based on what little we know about brain functioning. Dreyfus does not mention the flap associated with his RAND report’s release, which soon enough led to his relocating to the University of California at Berkeley, but he does note that it became his famous What Computers Can’t Do (Harper 1972, 1979), which was ultimately updated to What Computers Still Can’t Do in 1992 (ironically, published by MIT Press). In his preface to the MIT edition, he explains why he could be so sure that the symbolic processing approach of GOFAI defined what Imre Lakatos called a degenerating research program – one, that is, which shows great promise initially and on a small scale but is destined for internal design reasons never to achieve the grand successes predicted for it by its founders. The reason for his prescience, in a nutshell, is that he was already a phenomenologist.

In the intervening decades, many AI researchers – including some at MIT – came to think that Dreyfus’ criticism of GOFAI is basically correct, and some of them also concluded that overcoming his objections would require incorporating something of his Heideggerian approach to intelligence into AI programming. GOFAI’s main mistake, Dreyfus showed, is philosophical; hence, adding theoretical or engineering epicycles to GOFAI cannot save it. To refer back to Dennett’s account of the Cog project, making robots with mechanical bodies that at least appear humanoid cannot increase their ability to learn behaviors that rely on the embodied nature of perceptual life, or draw on the common-sense character of our everyday understanding – for the simple reason that human embodiment and common-sense understanding are nothing like, respectively, the physical state of a dolled-up artifact and a particular species of representing facts by rules. As described by Heidegger and Merleau-Ponty, being embodied, living spatio-temporally, and coping in a hundred ways with practical affairs constitutes a kind of everyday existence that is of an entirely different order from being a machine whose movements are directed by programmed coordinates on spatio-temporal maps and whose “thinking” is always some form of symbolic representation. Hence, GOFAI or any theory that assumes the activities of perception, thinking, and acting must somehow be “following rules” is doomed in advance, not for engineering, economic, or political reasons, but because one cannot “program” what has been ontologically misunderstood to begin with. Thus, in the present essay, Dreyfus criticizes Rodney Brooks’ 1990s empiricism for having essentially the same philosophical shortcomings as Marvin Minsky’s 1950s intellectualism, but in reverse. Where Minsky imagines symbolizing minds making sense out of a brute-factually given world, Brooks sees stimulated brains converting external input into reflexive responses. Both of them begin with the same traditional “us over here, world out there, organizing principles in the middle” metaphysics – which means they both miss the same point. Before traditional epistemologies of in-here and out-there can begin to analyze experience in third-person terms, split it up into a tripartite structure, and introduce theories to “explain” how the three parts “function” together, we are already living their intimate relatedness as “being-in-the-world,” and being globally responsive to anything and everything with no need for a separate set of organizing principles to get Us and the Other together. Indeed, as Dreyfus elsewhere, rationalist and empiricist reconstructivists betray their own recognition of this prior condition in never doubting that there is something already unified to reconstruct with their explanations – namely, the fact that “embodied beings like us take as input energy from the physical universe and respond in such a way as to open them to a world organized in terms of their needs, interests, and bodily capacities.”

Dreyfus goes on to discuss the work of Phil Agre and David Chapman, Michael Wheeler, and Walter Freeman, identifying the degree to which each of them seems to understand what Heidegger means by “openness to the world” and where their work still tends to fall back on an objectivist, or third-person, conception of what “must” be going on in human existence whether we “experience” it or not. Dreyfus’ main point is that being-in-the-world, as a more basic level of experience than cognizing and problem-solving, is not (yet) representational at all. Indeed, when we are busy coping with reasonable success, “we are drawn in by solicitations and respond directly to them, so that the distinction between us and our equipment vanishes.” On one issue, however, Dreyfus thinks that none of the researchers he discusses have made progress. Their use of Heidegger is focused entirely on the (perfectly admirable) purpose of developing better models of brain functioning in order to enable future computers/robots to be more responsive to their surroundings. But of course in human lives, this responsiveness cashes out in our understanding what to do, in determining what is significant for us and, in the first instance, recognizing the ways in which any situation presents us with matters to respond to at all. Hence, the best and most Heideggerian model of brain function would at most prepare an improved computer/robot for being responsive, but then we would still have to figure out how it could be programmed to actually pick out domains of significance – that is, as Dreyfus puts it, in some sense “understand” our “particular way of being embedded and embodied such that what we experience is significant for us in the particular way that it is.” The plain truth is, we are a long way from knowing how to put into computer programs “a model of a body very much like ours with our needs, desires, pleasures, pains, ways of moving, cultural background, etc.” It is in this context that Dreyfus refers to the so-called “frame problem.”

As noted above, the version of this problem that has become relevant in AI work involves figuring out theoretically something that humans do automatically and as a matter of common sense – something it is implausible to even imagine being a matter of applying specific rules to discrete facts – namely, determining not only what items of information about our surroundings hold constant for a certain a task, but selecting something as a task from the millions of stimuli we take in just by being in the midst of life. It is this capacity to “learn to act intelligently in our world” in a situation- or task-discriminating fashion that Dreyfus refers to at the end of his essay, when he says he already knows that the detailed models of human bodies and brains dreamed up in the “wild imaginations of a Ray Kurzweil or Bill Joy … haven’t a chance of being realized in the real world.”

What philosophers of technology can take away from Dreyfus’ discussion, then, is the argument that the promise of a research program is implicitly settled in advance by the relative phenomenological richness of its initial understanding of human lifeworld experience. For someone like Minsky or Dennett, who “already know” how AI research must proceed, Dreyfus’ argument must of course sound like little more than the expression of naïve preference for some sort of “unscientific” conception of consciousness instead of a “scientific” one, and they will read his subsequent willingness to concede that today’s computers can do things he argued in the 1960s they could “never” do as simply his “moving the goalposts” the way theologians used to do when they were fighting rearguard actions against scientific advances, especially in the study of humans. In fact, however, Dreyfus is not attacking the “science” but the “philosophy of science” of tradition-bound, still often symbolic representationalist AI. And here, the AI debates join with debates over building and using technologies more generally.

For in the eyes of philosophers like Dreyfus (and Heidegger and many phenomenologists), starting a philosophical investigation of today’s technologies by focusing on design issues and outcomes assessment – or, to take another example, by tacking on “ethical” questions or “political” concerns at the end of the analysis of ongoing technoscientific activity – comes too late and remains impotent in the face of whatever implicit (usually a late modern, empiricist-positivist version of traditional) philosophical understanding already informs the ongoing activity. This is why Heidegger and phenomenologists quite generally want their descriptions of everyday being-in-the-world to proceed without first seeking approval from the natural and human sciences. It is not that they mistakenly see themselves as producing another theory to compete with a “really scientific” approach to nature and humankind; rather, they are concerned that the philosophically primitive condition of the current understanding of nature and humankind will stunt the growth of the research programs it guides – as, for example, Dennett does when he takes it as “just obvious” that we “are” a kind of robot, because he is philosophically wedded to the idea that only what is physical and can be observed from a third-person viewpoint is “real.” Of course, to what extent the numerous competing phenomenologies and postphenomenologies of being-in-the-world deserve the status claimed for them is an open question.

In her famous “Cyborg Manifesto,” Donna Haraway suggests that the popular science fiction portrayals of cyborgs as part-human and part-machine reflect a general blurring in our age of the traditional boundary lines not only between persons and hardware but also, thanks to biotechnology, even between species. The implication, of course, is that this change in our ideas of who we are goes hand in hand with changes in our understanding of how we relate to each other and to nature. The general process of blurring all the old species-defining lines, she argues, is a manifestation of the global dominance of an information economy, and it now issues in a condition that many call postmodern. Postmodernism and poststructuralism reject the standard notions of a unitary, “centered” human being or society, in this way reflecting an older European generation’s critiques of the unphenomenological character of traditional conceptions of our existence. Such conceptions – like the idea of an essential self or “subject” in Descartes, Kant, and the Romantics, as well the idea of a society functioning according to some central dynamic like “labor,” as in Marx – all belong to an earlier, modern era. (Especially in recent French thought regarding the “decentering” of the self, the psychoanalytic theories of Freud and Lacan are widely regarded as pioneering sources; and the idea of a decentered social totality is traced above all to the Marxist structuralist, Louis Althusser.) Haraway argues that essentialist feminism, according to which Woman has some unchanging (usually inferior/subordinate) nature, is just as dated and constrictive as are the older humanisms that posit an essence of Man. Drawing on the writings of multicultural feminists, she proposes a view of selves and societies in which the intersection and crossing of lines of class, gender, and race result in non-unitary systems of overlapping allegiances and partial “identities.” Just as boundaries between selves, ethnicities, sexes, and species are erased in science fiction’s cyborgs, so also are such boundaries being erased in the rapidly developing multiethnic, trans-sexual, bioengineered condition of our time. It is interesting to note that Haraway’s conception of the blurring of lines between human and inorganic as well as between nature and culture implicitly supersedes Merchant’s account of the idea of nature as Nurturing Mother being replaced by another idea of nature as a purposeless object of exploitation. For Haraway, both ideas are still presented in too holistic and essentialist a way to reflect our present circumstances. Her article closes with the much-discussed, anti-essentialist remark, “I would rather be a cyborg than a [traditionally understood] goddess.”

Like Haraway, Selinger and Engström ponder both the accuracy and the social and moral value of all the currently fashionable talk about humans as cyborgs. Whereas Dreyfus (above) objects to the lingering influence of the symbolic representationalist or computational theory of mind (CTM) in robotics and cyborg engineering projects, they worry more about the way our ubiquitous use of technologies (in Selinger and Engström’s case, specifically digital devices) is increasingly being conceived in the same reductive terms. Thus, “wired and digitized” no longer just describes how we move around in the world; it threatens to form our fundamental conception of who and what we are.

Selinger and Engström find examples of this reductive slide into “cyborg ontology” in philosophy, science studies, marketing, and popular journalism. In philosophy, for example, Andy Clark’s updated version of David Chalmers’ theory of “extended mind” defines human beings as “reasoning systems whose minds and selves are spread across biological brain and nonbiological circuitry” – in other words, as “cognitive opportunists” with computational minds that “merge” with “nonbiological tools” to “solve problems.” Whatever issues Dreyfus thinks this ontologically pinched vision of humans might cause in research, Selinger and Engström worry that these issues will be seriously compounded as commercialized, popularized, and politicized versions of this vision come out of the laboratory and start to dominate our general sense of everyday life.

Clark’s definition of being human of course echoes Engels’ classical notion of humans as tool-users and toolmakers, but it is more crudely instrumentalist and apolitical, and these are the characteristics that make Clark’s theory possibly dangerous. Dreyfus is concerned to show that the era is drawing to a close when the philosophical enthusiasms of the Marvin Minskys, Steven Pinkers, and Daniel Dennetts of the profession can help make CTM the background understanding of choice for AI research programs. But Selinger and Engström think that CTM is actually on the rise as a cultural understanding of our everyday selves, of the artifacts we use, and of their relationship – and all the more so because there is commercial gold and political power in getting people to accept it. They cite, for example, both a “cyber-evangelist,” Kevin Warwick – who (like Nick Bostrom) hypes a posthuman future in which we will technologically “upgrade the human form” – and a self-identified “consumer anthropologist,” Markus Giesler – who extrapolates from Clark’s theory to promote a “posthumanist epistemology of technology consumption.” Giesler especially is no scientific researcher in search of knowledge. He wants to sell his epistemology to market researchers as a replacement for their more traditional Cartesian, representational epistemologies, so that they might better exploit the “meaning and experiences of technological consumption.” After all, a Cartesian epistemology – with its rigid subject–object dualism and old-fashioned philosophical anthropology of thinking selves – discourages marketers and consumers alike from appreciating, for example, that one’s relationship with an iPhone is a no longer a matter of me-using-an-artifact. It is a “merging” in which humans and machines form “cybernetic units,” and talking to Siri (Apple tells you in its blurbs that this is your “intelligent personal assistant”) is a “conversation.”

Perhaps most disturbing to Selinger and Engström, however, is that, from this “cyborg ontology,” certain obvious political conclusions fall out. Warwick and his colleagues are not only enthusiastic supporters of the bodily implantation of electronic communication devices, but also missionaries for a consumerist political economy in which corporate Big Brother is a friendly helper, monitoring our choices and marketing devices to us accordingly (what one critic has called the principle of Google’s Web Analytics, on steroids). Here the technological humanist present, where humans still think of themselves as tool-users and tool-makers, gives way to a cyber-utopian future, where we will be tool-made and tool-used. Selinger and Engström thus conclude by calling for a moratorium on cyborg talk. It is, they say, “political economy through the back door” – one whose political, social, and ethical implications never even come up for discussion, even though it quite clearly and strongly promotes a “worldview … that attempts to tell us not just who or what we are, but … who and what we should want to be.” Though the stakes are high and cyber-utopia surely an issue, they leave open the question of who might be persuaded to listen to their call, or why.

Finally, in “Anonymity versus Commitment” (revised later into a chapter in On the Internet (2001, 2009)), Dreyfus weaves a cautionary tale about the unintended effects of our ever-growing reliance on the Internet. Like Selinger and Engström, he is concerned about the way current technologies are shaping our conceptions of the good life. In this essay, he expresses his concern by ingeniously rethinking our experience of the Internet in terms of Søren Kierkegaard’s critique of “The Press.” Kierkegaard, the nineteenth-century existentialist and religious thinker, describes the popular press of his time as driven by an urge to satisfy the insatiable demands of its readers for “information” to indiscriminately report every “interesting” new event – thus encouraging a dubious sort of uncommitted attitude of curiosity in which everything is momentarily interesting but nothing seems really worth knowing. In Kierkegaard’s language, “The Press” creates “The Public.” The trouble with being curious, of course, is that ultimately the only thing that can keep boredom at bay is yet another round of something “new.” Here Dreyfus sees similarities between Kierkegaard’s critique of the dissemination of “news” by The Press and his own misgivings about the endless availability of information on the Internet. But the bulk of his discussion is directed toward a provocative counter-suggestion. With a creative reinterpretation of Kierkegaard’s “three stages on life’s way,” he argues that the experience of having such an excess of information would most certainly encourage us to live aesthetically, like an undirected and idly curious Web-surfer. It might also lead us to discover the implicit flight from boredom that the life of an aesthete implies and thus drive us toward Kierkegaard’s ethical sense of life, where we would try to adhere to a “principled” conversion of all information into something that may or may not be worth selecting for the sake of knowledge from a “chosen” perspective.

What is much less likely, however, is that our experiences on the Internet will encourage us to see the implicit nihilism in the ethical sense of life, where “choosing the principle of our choices” is of course itself nothing at bottom but a choice. And without recognizing this limitation in ethical choice, there is even less chance that processing all that Internet information will lead us to understand the difference between mere choice and the sort of genuine choice which Kierkegaard identifies with the religious stage (but which Dreyfus construes in a secular fashion) – that is, the kind of choice that turns an otherwise merely optional perspective into a genuine and risk-filled “commitment.”