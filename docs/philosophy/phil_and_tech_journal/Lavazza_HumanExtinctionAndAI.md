Title: Human Extinction and AI: What We Can Learn from the Ultimate Threat | Philosophy & Technology
URL: https://link.springer.com/article/10.1007/s13347-024-00706-2
Content:

1 Introduction: Current Existential Risks
-----------------------------------------

That the human species might become extinct is a prospect that has only recently started to surface often, although the idea of a cyclical destruction and rebirth of a civilization – if not the universe as a whole – is an ancient one, recurring in many different cultures. For example, dates marking a new millennium, such as the year 1000, were often viewed as potentially signifying the end of the world. That being noted, it seems to be safe to say that the idea of human extinction spread mainly in the twentieth century because of the fear of a thermonuclear war, which could have caused the destruction of life on Earth or at least the death of most if not all human beings.[Footnote 1] More recently, climate change – caused by human activity – has shown that there is a risk of the Earth becoming uninhabitable for humans. The pandemic outbreak due to COVID-19 has also revived fears[Footnote 2] that a deadly virus will spread to the point of annihilating the human species.[Footnote 3] In Ord’s terms, it seems that we are on the edge of a precipice and that the future of humanity is extremely uncertain (Ord, 2020).

However, it should not be forgotten that despite the awareness of the risks humanity is running today, some positions suggest that we can look to the future of humankind with optimism.[Footnote 4] According to Bostrom, for example, it is dangerous to be alive, but luckily not all risks are serious (Bostrom, 2002). In his view, the magnitude of risks can vary in scope (e.g., “the size of the group of people that are at risk”), intensity (e.g., “how badly each individual in the group would be affected”) and probability (e.g., “the best current subjective estimate of the probability of the adverse outcome”) (Bostrom, 2002). Their central theses are that our moral psychology does not evolve fast enough; that we are unfit for the future; that social measures are insufficient and that a moral bioenhancement is necessary and urgent so that we can avoid the risk of extinction caused by human action itself (Persson & Savulescu, 2012).

In the face of these threats, which are largely the result of human activity, the prospect of _Homo sapiens_' extinction suggests the need to think about how we might act pre-emptively. In this article, we will focus on a necessarily hypothetical scenario that is made as realistic as possible. However, our purpose is primarily to present some ethical considerations rather than to provide detailed technological detail related to digital duplicates and machine learning application.

The premise, therefore, is that there is an impending threat and that there is a will to address the issue of our potential extinction (cf. MacAskill, 2022). Faced with a worsening of climatic conditions, or the repeated occurrence of epidemics caused by increasingly aggressive and incurable viruses, as well the possible misuses of science and technology (such as the creation of lethal biological agents in the lab; the uncontrolled release of genetically modified species into the environment or unsuccessful attempts to reverse climate change—for example, the excessive release of silver iodide into the atmosphere to seed clouds), it might be worth starting to contemplate a way to pass what is the best in the human species to what might be our “heirs”, i.e., entities which we will describe below.

According to pessimists about the development of artificial intelligence (cf. Bostrom, 2014). Kahane's argument holds as a premise that a thing is important to the extent that it contributes to the overall intrinsic value of the domain in which it is found. If there is only one thing of intrinsic value in the universe, then the entire value of the universe will be given by that thing, which is the thing that makes the greatest difference to the value of the universe. The other fundamental assumption is that Terrestrial life has intrinsic value.

In Kahane’s words, “terrestrial life as shorthand for all the value associated with sentient life on our planet, from the pains and pleasures of dormice to the horrors and triumphs of human history. Different axiologies will develop the details differently. For our purposes it is enough that nearly everyone accepts some version of \[the claim that terrestrial life has intrinsic value\] − even pessimists, who think that this value is negative” (Kahane, 2021).[Footnote 8] Indeed, we think that in the light of the existential risks humanity faces and the hypothesis of extinction, the various ways in which artificial intelligence can help to deal with this eventuality are to be welcomed. In addition, as will emerge from the development of our argument, the application of machine learning to the prospect of extinction allows us to appreciate how (some forms of) decision-making automation can be of great help even in less extreme situations (Moravec, 1988). Of course, this does not exempt us from carefully considering all ethical issues related to the massive application of artificial intelligence in our lives (Floridi, 2023).

Given the rapid development of artificial intelligence, instead of artificial successors to humans, one might think of using such advanced technology to avoid an existential catastrophe caused by climate change or pandemics, in ways that we cannot even think of right now. This objection makes sense, but it is plausible to assume that climate change, having reached a certain stage, is not reversible quickly enough to avoid the extinction of many living forms, including humans. A super-intelligent AI might suggest “unthinkable” remedies, but they might not necessarily be physically implementable globally in time. The same could be said for a pandemic. Unless the superAI finds an effective vaccine or drug, and this is not certain to be the case for every possible virus, it does not seem likely that an unseen intelligence could save us from contagion. Furthermore, a super intelligence is such in comparison to the intelligence of a human being, but it does not mean omniscience and omnipotence as God ideally has.

That said, how to create successors that are sentient (conscious) or at least capable of rational, human-like behaviour is not a simple problem to address. So-called techno-optimism about machine consciousness claims that when a very sophisticated general-purpose AI is developed, then such AI will be conscious. But as Schneider well explains, this position, called ‘computationalism’, is based on very strong assumptions, which are far from obvious. Computationalism holds that one can explain a cognitive or perceptual ability by breaking it down into causally acting parts if each part is describable according to a specific algorithm (Schneider, 2019). It follows that thought is independent of the substrate in which it tak es place. In this sense, if we can reproduce the internal computational configuration of an entity that we know to be conscious, then we will have an accurate isomorph of it, which will be as conscious as the entity that was reproduced. However, what is conceptually possible is not necessarily technologically feasible, and beyond all possible theoretical objections to computationalism, it seems that we are a long way from being able to build that kind of specific isomorphs of humans at present.

Not even the thought experiment involving the progressive replacement of each human biological neuron with silicon chips tells us much more in this regard. It is a logical-conceptual possibility but remains a perhaps insurmountable technical-material issue. In fact, it is not to be excluded that artificial consciousness might rather arise from supercomputers that are not mere copies of individuals, and the latter might not be sufficiently numerous to constitute something like a population.

A more interesting hypothesis is instead that artificial intelligence will develop very advanced cognitive capabilities, way superior to those available to humans so far, even with the help of technical tools. In other words, even our extended minds would be far inferior to next-generation artificial intelligence (Clark & Chalmers, 1998), and such an AI is perhaps not that far off in time. If one of the purposes of our consciousness is that of allowing us to focus attention, to grasp the salience of events or environments based on perceived emotions, or to facilitate other functions useful to the flourishing of the species, humanoid robots equipped with next-generation artificial intelligence might not even need consciousness to accomplish all that humans achieve thanks to the phenomenology they experience.

On the other hand, one possible test for measuring consciousness in machines, devised by Schneider and Turner ([2017](https://link.springer.com/article/10.1007/s13347-024-00706-2#ref-CR44 "Schneider, S., & Turner, E. (2017). Is anyone home? A way to find out if AI has become self-aware. Scientific American. Retrieved from 
https://blogs.scientificamerican.com/observations/is-anyone-home-a-way-to-find-out-if-ai-has-become-self-aware
. Accessed Dec 2023")), allows us to identify some explicit behavioural features in the machines themselves that might be indicators of consciousness as we experience it. The presence of those indicators, however, would not necessarily erase the doubt that intelligent machines are simply zombies (Chalmers, 1996). Imposing a kind of ethical determinism through the biochemical enhancement of humans would certainly conflict with some of our basic moral intuitions. However, it has been argued that this objection is not definitive, and one can justify moral enhancement in the direction of prosocial behaviour (Douglas, 2013).

In the scenario of artificial successors to humans, which would be potentially programmable in a certain way, the terms of the question seem to change. On the one hand, no possible comparisons could be made. All individuals would have a predominant prosocial tendency because the humanoid robots would have been instructed not to assimilate human anti-social tendencies, or to exhibit prosocial behaviours and inhibit anti-social ones, in a more sophisticated version of Asimov's laws. There would be no limitations on freedom, except from a hypothetical point of view, in reference to the extinct humans. On the other hand, the evolutionary dynamics of artificial intelligence, capable of learning, may not exclude the emergence of unanticipated behaviours. An environment that has become hostile for humanoid robots could lead to competition for scarce resources (energy), and even interaction with the non-human living forms left on Earth might trigger new attitudes, including aggressive and predatory ones.

It is here that machine learning can play a key role. Notoriously, machine learning a subfield of artificial intelligence, has emerged as a transformative approach for extracting valuable insights and making data-driven predictions. By leveraging algorithms and statistical models, machine learning enables computer systems to automatically learn and improve from experience without explicit programming.

Based on a combination of selected human features based on machine learning, our artificial successors would, at least potentially, be morally better than us. Yet, the entirety of the human heritage would remain inscribed in each android robot- recall that Klara is indistinguishable from Josie even in the eyes of the latter’s parents. This circumstance means that human culture would continue to produce its effects and affect the evolution of the new society made up of artificial individuals.

This opens another very interesting issue, related to the selection of human individuals to replicate. In fact, it seems unlikely that billions of humanoid robots could be built with sophisticated microprocessors capable of implementing an advanced level of artificial intelligence. So, who should be replicated? Who are the best candidates to transition into the new species? Only young people, for example? Should criminals be excluded? Who should decide? These are questions that refer to the kind of society we would like to create, even if we are fully aware that we will not be there to witness it and that we cannot reasonably think of truly guiding its evolution.

Obviously, many people would aspire to have a successor, in the belief that they deserve it and would be useful to the future society, or simply because they would like to continue living in another form, as happens with physical procreation and cultural transmission between parents and children. Well-off individuals might have easier access to artificial successors, but the authorities might choose to implement fair criteria for allocating scarce resources. Would a society of artificial successors benefit from diversity or homogeneity of values and cultures? Or should variety be preferred as a function of adaptation to a changing environment? Indeed, if we are to adhere to the idea that sentient entities are the ones with the most value and significance, then we should favour those with the richest personal phenomenology.

How can we identify such individuals? They might be writers, or artists in general, who can express a wide range of feelings and emotions. Or they might be individuals who have had many experiences in their life: this would mean privileging the elderly over the younger, or those who have suffered more adversity in their life than those who have led quiet, uneventful lives. We mentioned that humanoid robots would be intelligent but not conscious. However, the evolution of AI coupled with the wealth of experience assimilated by the humanoid robots could lead it to manifest the behaviours that appear to be the result of consciousness, even if one could not reliably establish whether the humanoid robot has indeed become conscious. This is not to adopt a behaviourist perspective on consciousness and the entities under examination, but merely to remain agnostic about whether AI might become conscious under specific conditions and what this development might look like.

As mentioned above, in addition to successors of particular individuals, one could have successors of human beings without direct continuity with living individuals but realized on the basis of average psychological functioning with the addition of specific characteristics selected by means of the algorithmic procedure.

In the scenario just outlined, the use of machine learning would become a form of choosing the preferred characteristics to be inserted in our robotic duplicates and which individual to prioritize. The automated procedures per se are not always a guarantee of an unbiased procedure and results. Both data on which systems are trained and humans who designed them can be carriers of biases, errors, and prejudices (cf. Crawford, 2021). Yet, the urgency and difficulty of the task of selecting human features to be passed down through our digital heirs and such as to enable the optimal development of the artificial duplicate society would make one resort to a process that is as natural as possible.

Faced with such different cultures, ideologies, religions, worldviews, and material interests that characterise the societies inhabiting the Earth, it seems difficult to find a shared process that could lead to outcomes easily accepted by at least most individuals (consider also the extreme condition of deciding in the light of a possible mass extinction).

The use of machine learning algorithms may then be an 'external' and shared solution. From the efforts of the best experts in charge of implementing such a machine learning system, 'supervised' if one may say so by the entire world community, one can reasonably expect a selection of data and a procedure that is least influenced by previous biased assessments. The usefulness of resorting to machine learning will thus be to have the trained system extract as neutrally as possible the elements and characteristics most suited to our successors and most reflective of 'our better angels'.

There is obviously an axiological and normative dimension to all this. But where it cannot easily be adjudicated by a classical deliberative procedure—representative assemblies, voting, other forms of preference ordering—then machine learning may become the most effective available modality.

It is well known that machine learning involves the process of training a computer system to recognize patterns in data and make predictions or take actions without explicit programming (Flach, 2012), can also avoid the biases that often characterize expert decisions (Kahneman et al., 2021). The algorithm should then solve the task given to it by humans: what characteristics should our successors need to have for they to be the best expression of the value of terrestrial life and able to survive under the conditions of biological extinction? That should be done based on what humans have accomplished so far but without the cognitive and emotional limitations of humans themselves.[Footnote 14] This criterion of efficiency sacrifices the other values at stake in the democratic procedure, but we could democratically choose to rely on the algorithm when deciding what our successors should look like. In case of biological extinction, we would in fact have no possibility to modify the choice. And a wrong choice about the successors could have the result of compromising the continuation of the value of terrestrial life.

In this vein, it is important to stress the incorporation of some features of the so-called trustworthy AI (European Commission, 2019) into the chosen algorithm. The first concept is that of interpretability/explainability to have the possibility to visualize the estimated or found relations among variables (Allgaier et al., 2023). The second concept is that of fairness, which means not discriminating against specific groups or individuals, mainly based on not biased data (Pessach & Shmueli, 2022). And, finally, the last concept to be considered is that of generalizability (or external validity), the ability of a trained model to perform well on unseen or new data that it has not encountered during the training process. A model with high generalizability is able to make accurate predictions or classifications on diverse and previously unseen examples (Maleki et al., 2022).

So, what could be called the ‘ultimate algorithm’ should be the result of an investment in research that overcomes objections to the use of algorithms for decisions that affect society as a whole.

6 Conclusion
------------

Faced with the direct and perceptible threat of the extinction of the human species, the survival instinct that evolution has inscribed in us leads us to make every effort to avoid such an outcome. But the undesirable compound effects of many of our behaviours might bring us to the brink of extinction without it being possible to rectify our past choices. This is the result of cognitive and moral limitations combined with the strength and pervasiveness of the technical tools we have at our disposal today. If we believe that the human species – made up of individuals capable of consciousness and rationality – is something of value worth preserving in the form of artificial successors, the only ones capable of surviving in a modified environment that is inhospitable to us (climatically transformed or populated by incurable viruses), then we can ask ourselves how such successors should be conceived and constructed.

In this article we have argued that one hypothesis would be to copy our mental functions into advanced humanoid robots in line with the fictional scenario envisaged in Ishiguro's novel _Klara and the Sun_. The philosophical discussion of this hypothetical situation led us to consider how best to select the characteristics to be favoured in such artificial successors for them to flourish as individuals and as a society. The difficulty of such a choice led us to consider that such a procedure could be entrusted to an evolved algorithm. If this might be a feasible way forward in the ultimate threat scenario, we might also deem it to be a viable solution even _before_ the ultimate threat manifests itself; indeed, we might regard it as a possibility to _avoid_ the ultimate threat of our species’ extinction.

Instead of morally enhancing individuals through biotechnology, as has been suggested, we could ‘enhance’ our conduct by relying on well-designed algorithms to guide our choices towards our chosen ends, primarily the survival of the species in the face of the threats of human-induced climate change and the spread of deadly zoonotic viruses due to the uncontrolled anthropization of parts of the planet. It is a question of avoiding the compound effects and the inability to find shared solutions at the political and social level due to the weakness of will at the individual level.

We can therefore state that the prospect of the extinction of _Homo sapiens_, which forces us to envision extreme scenarios, can teach us a great deal about the here and now. Indeed, if extinction does not occur, one can think that the next generations – i.e., our biological successors – would benefit from considering – as we did here—more efficient algorithmic procedures, without violating, indeed perhaps enhancing, those values whose exercise qualifies us as moral beings.

References
----------

*   Allgaier, J., Mulansky, L., Draelos, R. L., & Pryss, R. (2023). How does the model make predictions? A systematic literature review on the explainability power of machine learning in healthcare. _Artificial Intelligence in Medicine,_ _143_, 102616.
    
    [Article](https://doi.org/10.1016%2Fj.artmed.2023.102616)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=How%20does%20the%20model%20make%20predictions%3F%20A%20systematic%20literature%20review%20on%20the%20explainability%20power%20of%20machine%20learning%20in%20healthcare&journal=Artificial%20Intelligence%20in%20Medicine&doi=10.1016%2Fj.artmed.2023.102616&volume=143&publication_year=2023&author=Allgaier%2CJ&author=Mulansky%2CL&author=Draelos%2CRL&author=Pryss%2CR) 
    
*   Appel, M., Izydorczyk, D., Weber, S., Mara, M., & Lischetzke, T. (2020). The uncanny of mind in a machine: Humanoid robots as tools, agents, and experiencers. _Computers in Human Behavior,_ _102_, 274–286.
    
    [Article](https://doi.org/10.1016%2Fj.chb.2019.07.031)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=The%20uncanny%20of%20mind%20in%20a%20machine%3A%20Humanoid%20robots%20as%20tools%2C%20agents%2C%20and%20experiencers&journal=Computers%20in%20Human%20Behavior&doi=10.1016%2Fj.chb.2019.07.031&volume=102&pages=274-286&publication_year=2020&author=Appel%2CM&author=Izydorczyk%2CD&author=Weber%2CS&author=Mara%2CM&author=Lischetzke%2CT) 
    
*   Benatar, D. (2008). _Better never to have been: The harm of coming into existence_. Oxford University Press.
    
    [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Better%20never%20to%20have%20been%3A%20The%20harm%20of%20coming%20into%20existence&publication_year=2008&author=Benatar%2CD) 
    
*   Bostrom, N. (2002). Existential risks: Analyzing human extinction scenarios and related hazards. _J Evol Technol,_ _9_(1), 1–29.
    
    [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Existential%20risks%3A%20Analyzing%20human%20extinction%20scenarios%20and%20related%20hazards&journal=J%20Evol%20Technol&volume=9&issue=1&pages=1-29&publication_year=2002&author=Bostrom%2CN) 
    
*   Bostrom, N. (2014). _Superintelligence: Paths, dangers, strategies_. Oxford University Press.
    
    [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Superintelligence%3A%20Paths%2C%20dangers%2C%20strategies&publication_year=2014&author=Bostrom%2CN) 
    
*   Chalmers, D. J. (1996). _The conscious mind: In search of a fundamental theory_. Oxford University Press.
    
    [Google Scholar](http://scholar.google.com/scholar_lookup?&title=The%20conscious%20mind%3A%20In%20search%20of%20a%20fundamental%20theory&publication_year=1996&author=Chalmers%2CDJ) 
    
*   Chalmers, D. J. (2023). Does thought require sensory grounding? From pure thinkers to large language models. _Proceedings and Addresses of the American Philosophical Association, 97_, 22–45. [https://philpapers.org/archive/CHADTR.pdf](https://philpapers.org/archive/CHADTR.pdf)
    
*   Christov-Moore, L., Reggente, N., Vaccaro, A., Schoeller, F., Pluimer, B., Douglas, P. K., Iacoboni, M., Man, K., Damasio, A., & Kaplan, J. T. (2023). Preventing antisocial robots: A pathway to artificial empathy. _Science Robotics,_ _8_(80), eabq3658.
    
    [Article](https://doi.org/10.1126%2Fscirobotics.abq3658)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Preventing%20antisocial%20robots%3A%20A%20pathway%20to%20artificial%20empathy&journal=Science%20Robotics&doi=10.1126%2Fscirobotics.abq3658&volume=8&issue=80&publication_year=2023&author=Christov-Moore%2CL&author=Reggente%2CN&author=Vaccaro%2CA&author=Schoeller%2CF&author=Pluimer%2CB&author=Douglas%2CPK&author=Iacoboni%2CM&author=Man%2CK&author=Damasio%2CA&author=Kaplan%2CJT) 
    
*   Ćirković, M. M., Sandberg, A., & Bostrom, N. (2010). Anthropic shadow: Observation selection effects and human extinction risks. _Risk Anal,_ _30_(10), 1495–1506. [https://doi.org/10.1111/j.1539-6924.2010.01460.x](https://doi.org/10.1111/j.1539-6924.2010.01460.x)
    
    [Article](https://doi.org/10.1111%2Fj.1539-6924.2010.01460.x)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Anthropic%20shadow%3A%20Observation%20selection%20effects%20and%20human%20extinction%20risks&journal=Risk%20Anal&doi=10.1111%2Fj.1539-6924.2010.01460.x&volume=30&issue=10&pages=1495-1506&publication_year=2010&author=%C4%86irkovi%C4%87%2CMM&author=Sandberg%2CA&author=Bostrom%2CN) 
    
*   Clark, A., & Chalmers, D. (1998). The extended mind. _Analysis,_ _58_(1), 7–19. [https://doi.org/10.1111/1467-8284.00096](https://doi.org/10.1111/1467-8284.00096)
    
    [Article](https://doi.org/10.1111%2F1467-8284.00096)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=The%20extended%20mind&journal=Analysis&doi=10.1111%2F1467-8284.00096&volume=58&issue=1&pages=7-19&publication_year=1998&author=Clark%2CA&author=Chalmers%2CD) 
    
*   Clarke, S., Zohny, H., & Savulescu, J. (Eds.). (2021). _Rethinking moral status_. Oxford University Press.
    
    [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Rethinking%20moral%20status&publication_year=2021) 
    
*   Crawford, J. (2010). _Confessions of an antinatalist_. Nine-Banded Books.
    
    [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Confessions%20of%20an%20antinatalist&publication_year=2010&author=Crawford%2CJ) 
    
*   Crawford, K. (2021). _Atlas of AI. Power, politics, and the planetary cost of artificial intelligence_. Yale University Press.
    
    [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Atlas%20of%20AI.%20Power%2C%20politics%2C%20and%20the%20planetary%20cost%20of%20artificial%20intelligence&publication_year=2021&author=Crawford%2CK) 
    
*   Douglas, T. (2013). Moral enhancement via direct motion modulation: A reply to John Harris. _Bioethics,_ _27_(3), 160–168. [https://doi.org/10.1111/j.1467-8519.2011.01919.x](https://doi.org/10.1111/j.1467-8519.2011.01919.x)
    
    [Article](https://doi.org/10.1111%2Fj.1467-8519.2011.01919.x)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Moral%20enhancement%20via%20direct%20motion%20modulation%3A%20A%20reply%20to%20John%20Harris&journal=Bioethics&doi=10.1111%2Fj.1467-8519.2011.01919.x&volume=27&issue=3&pages=160-168&publication_year=2013&author=Douglas%2CT) 
    
*   European Commission, Directorate General for Communications Networks, Content and Technology, High-Level Expert Group on Artificial Intelligence. (2019). _Ethics guidelines for trustworthy AI_. Publications Office. [https://data.europa.eu/doi/10.2759/177365](https://data.europa.eu/doi/10.2759/177365). Accessed Dec 2023
    
*   Farina, M., & Lavazza, A. (2023). ChatGPT in society: Emerging issues. _Frontiers in Artificial Intelligence,_ _6_, 1130913. [https://doi.org/10.3389/frai.2023.1130913](https://doi.org/10.3389/frai.2023.1130913)
    
    [Article](https://doi.org/10.3389%2Ffrai.2023.1130913)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=ChatGPT%20in%20society%3A%20Emerging%20issues&journal=Frontiers%20in%20Artificial%20Intelligence&doi=10.3389%2Ffrai.2023.1130913&volume=6&publication_year=2023&author=Farina%2CM&author=Lavazza%2CA) 
    
*   Flach, P. (2012). _Machine learning: The art and science of algorithms that make sense of data_. Cambridge University Press.
    
    [Book](https://doi.org/10.1017%2FCBO9780511973000)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Machine%20learning%3A%20The%20art%20and%20science%20of%20algorithms%20that%20make%20sense%20of%20data&doi=10.1017%2FCBO9780511973000&publication_year=2012&author=Flach%2CP) 
    
*   Floridi, L. (2023). _The ethics of artificial intelligence_. Oxford University Press.
    
    [Book](https://doi.org/10.1093%2Foso%2F9780198883098.001.0001)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=The%20ethics%20of%20artificial%20intelligence&doi=10.1093%2Foso%2F9780198883098.001.0001&publication_year=2023&author=Floridi%2CL) 
    
*   Goodfellow, I., Bengio, Y., & Courville, A. (2016). _Deep learning_. The MIT Press.
    
    [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Deep%20learning&publication_year=2016&author=Goodfellow%2CI&author=Bengio%2CY&author=Courville%2CA) 
    
*   Harris, J. (2011). Moral enhancement and freedom. _Bioethics,_ _25_(2), 102–111. [https://doi.org/10.1111/j.1467-8519.2010.01854.x](https://doi.org/10.1111/j.1467-8519.2010.01854.x)
    
    [Article](https://doi.org/10.1111%2Fj.1467-8519.2010.01854.x)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Moral%20enhancement%20and%20freedom&journal=Bioethics&doi=10.1111%2Fj.1467-8519.2010.01854.x&volume=25&issue=2&pages=102-111&publication_year=2011&author=Harris%2CJ) 
    
*   Harris, J. (2016). _How to be good: The possibility of moral enhancement_. Oxford University Press.
    
    [Book](https://doi.org/10.1093%2Facprof%3Aoso%2F9780198707592.001.0001)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=How%20to%20be%20good%3A%20The%20possibility%20of%20moral%20enhancement&doi=10.1093%2Facprof%3Aoso%2F9780198707592.001.0001&publication_year=2016&author=Harris%2CJ) 
    
*   Hinton, G., Bengio, Y., Hassabis, D., Altman, S., Amodei, D., Song, D. et al. (2023). Statement on AI Risk. AI experts and public figures express their concern about AI risk. [https://www.safe.ai/statement-on-ai-risk](https://www.safe.ai/statement-on-ai-risk). Accessed Dec 2023
    
*   Ishiguro, K. (2021). _Klara and the sun_. Alfred A. Knopf.
    
    [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Klara%20and%20the%20sun&publication_year=2021&author=Ishiguro%2CK) 
    
*   Jeffries, S. (2021). The world’s first robot artist discusses beauty, Yoko Ono and the perils of AI. _The Spectator_. Retrieved from [https://www.spectator.co.uk/article/the-worlds-first-robot-artist-discusses-beauty-yoko-ono-and-the-perils-of-ai](https://www.spectator.co.uk/article/the-worlds-first-robot-artist-discusses-beauty-yoko-ono-and-the-perils-of-ai). Accessed Dec 2023
    
*   Kahane, G. (2014). Our cosmic insignificance. _Noûs,_ _48_(4), 745–772. [https://doi.org/10.1111/nous.12030](https://doi.org/10.1111/nous.12030)
    
    [Article](https://doi.org/10.1111%2Fnous.12030)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Our%20cosmic%20insignificance&journal=No%C3%BBs&doi=10.1111%2Fnous.12030&volume=48&issue=4&pages=745-772&publication_year=2014&author=Kahane%2CG) 
    
*   Kahane, G. (2021). Importance, value, and causal impact. _Journal of Moral Philosophy_. Advance online publication. [https://doi.org/10.1163/17455243-20213581](https://doi.org/10.1163/17455243-20213581)
    
*   Kahneman, D., Sibony, O., & Sunstein, C. R. (2021). _Noise: A flaw in human judgment_. Little, Brown Spark.
    
    [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Noise%3A%20A%20flaw%20in%20human%20judgment&publication_year=2021&author=Kahneman%2CD&author=Sibony%2CO&author=Sunstein%2CCR) 
    
*   Kurzweil, R. (2005). _The singularity is near: When humans transcend biology_. Viking.
    
    [Google Scholar](http://scholar.google.com/scholar_lookup?&title=The%20singularity%20is%20near%3A%20When%20humans%20transcend%20biology&publication_year=2005&author=Kurzweil%2CR) 
    
*   Leist, A. K., Klee, M., Kim, J. H., Rehkopf, D. H., Bordas, S. P., Muniz-Terrera, G., & Wade, S. (2022). Mapping of machine learning approaches for description, prediction, and causal inference in the social and health sciences. _Science Advances,_ _8_(42), eabk1942.
    
    [Article](https://doi.org/10.1126%2Fsciadv.abk1942)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Mapping%20of%20machine%20learning%20approaches%20for%20description%2C%20prediction%2C%20and%20causal%20inference%20in%20the%20social%20and%20health%20sciences&journal=Science%20Advances&doi=10.1126%2Fsciadv.abk1942&volume=8&issue=42&publication_year=2022&author=Leist%2CAK&author=Klee%2CM&author=Kim%2CJH&author=Rehkopf%2CDH&author=Bordas%2CSP&author=Muniz-Terrera%2CG&author=Wade%2CS) 
    
*   Lemos, N. M. (2015). Value. In R. Audi (Ed.), _The Cambridge dictionary of philosophy_ (pp. 1100–1101). Cambridge University Press.
    
    [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Value&pages=1100-1101&publication_year=2015&author=Lemos%2CNM) 
    
*   Lingam, M., & Loeb, A. (2019). Relative likelihood of success in the search for primitive versus intelligent extraterrestrial life. _Astrobiology,_ _19_(1), 28–39. [https://doi.org/10.1089/ast.2018.1936](https://doi.org/10.1089/ast.2018.1936)
    
    [Article](https://doi.org/10.1089%2Fast.2018.1936)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Relative%20likelihood%20of%20success%20in%20the%20search%20for%20primitive%20versus%20intelligent%20extraterrestrial%20life&journal=Astrobiology&doi=10.1089%2Fast.2018.1936&volume=19&issue=1&pages=28-39&publication_year=2019&author=Lingam%2CM&author=Loeb%2CA) 
    
*   MacAskill, W. (2022). _What we owe the future: A million-year view_. Oneworld Publications.
    
    [Google Scholar](http://scholar.google.com/scholar_lookup?&title=What%20we%20owe%20the%20future%3A%20A%20million-year%20view&publication_year=2022&author=MacAskill%2CW) 
    
*   Maleki, F., Ovens, K., Gupta, R., Reinhold, C., Spatz, A., & Forghani, R. (2022). Generalizability of machine learning models: Quantitative evaluation of three methodological pitfalls. _Radiology: Artificial Intelligence_, 5(1), e220028.
    
*   Martens, D. (2022). _Data science ethics: Concepts, techniques, and cautionary tales_. Oxford University Press.
    
    [Book](https://doi.org/10.1093%2Foso%2F9780192847263.001.0001)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Data%20science%20ethics%3A%20Concepts%2C%20techniques%2C%20and%20cautionary%20tales&doi=10.1093%2Foso%2F9780192847263.001.0001&publication_year=2022&author=Martens%2CD) 
    
*   Moravec, H. (1988). _Mind children: The future of robot and human intelligence_. Harvard University Press.
    
    [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Mind%20children%3A%20The%20future%20of%20robot%20and%20human%20intelligence&publication_year=1988&author=Moravec%2CH) 
    
*   Moynihan, T. (2020). Existential risk and human extinction: An intellectual history. _Futures,_ _116_, 102495. [https://doi.org/10.1016/j.futures.2019.102495](https://doi.org/10.1016/j.futures.2019.102495)
    
    [Article](https://doi.org/10.1016%2Fj.futures.2019.102495)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Existential%20risk%20and%20human%20extinction%3A%20An%20intellectual%20history&journal=Futures&doi=10.1016%2Fj.futures.2019.102495&volume=116&publication_year=2020&author=Moynihan%2CT) 
    
*   Murphy, T. F. (2016). What justifies a future with humans in it. _Bioethics,_ _30_(9), 751–758. [https://doi.org/10.1111/bioe.12290](https://doi.org/10.1111/bioe.12290)
    
    [Article](https://doi.org/10.1111%2Fbioe.12290)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=What%20justifies%20a%20future%20with%20humans%20in%20it&journal=Bioethics&doi=10.1111%2Fbioe.12290&volume=30&issue=9&pages=751-758&publication_year=2016&author=Murphy%2CTF) 
    
*   Ord, T. (2020). _The precipice: Existential risk and the future of humanity_. Hachette Book.
    
    [Google Scholar](http://scholar.google.com/scholar_lookup?&title=The%20precipice%3A%20Existential%20risk%20and%20the%20future%20of%20humanity&publication_year=2020&author=Ord%2CT) 
    
*   Persson, I., & Savulescu, J. (2008). The perils of cognitive enhancement and the urgent imperative to enhance the moral character of humanity. _Journal of Applied Philosophy,_ _25_(3), 162–177. [https://doi.org/10.1111/j.1468-5930.2008.00410.x](https://doi.org/10.1111/j.1468-5930.2008.00410.x)
    
    [Article](https://doi.org/10.1111%2Fj.1468-5930.2008.00410.x)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=The%20perils%20of%20cognitive%20enhancement%20and%20the%20urgent%20imperative%20to%20enhance%20the%20moral%20character%20of%20humanity&journal=Journal%20of%20Applied%20Philosophy&doi=10.1111%2Fj.1468-5930.2008.00410.x&volume=25&issue=3&pages=162-177&publication_year=2008&author=Persson%2CI&author=Savulescu%2CJ) 
    
*   Persson, I., & Savulescu, J. (2012). _Unfit for the future: The need for moral enhancement_. Oxford University Press.
    
    [Book](https://doi.org/10.1093%2Facprof%3Aoso%2F9780199653645.001.0001)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Unfit%20for%20the%20future%3A%20The%20need%20for%20moral%20enhancement&doi=10.1093%2Facprof%3Aoso%2F9780199653645.001.0001&publication_year=2012&author=Persson%2CI&author=Savulescu%2CJ) 
    
*   Pessach, D., & Shmueli, E. (2022). A review on fairness in machine learning. _ACM Computing Surveys (CSUR),_ _55_(3), 1–44.
    
    [Article](https://doi.org/10.1145%2F3494672)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=A%20review%20on%20fairness%20in%20machine%20learning&journal=ACM%20Computing%20Surveys%20%28CSUR%29&doi=10.1145%2F3494672&volume=55&issue=3&pages=1-44&publication_year=2022&author=Pessach%2CD&author=Shmueli%2CE) 
    
*   Russell, S. (2019). _Human compatible: Artificial intelligence and the problem of control_. Viking.
    
    [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Human%20compatible%3A%20Artificial%20intelligence%20and%20the%20problem%20of%20control&publication_year=2019&author=Russell%2CS) 
    
*   Schneider, S. (2019). _Artificial you: AI and the future of your mind_. Princeton University Press.
    
    [Book](https://doi.org/10.1515%2F9780691197777)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Artificial%20you%3A%20AI%20and%20the%20future%20of%20your%20mind&doi=10.1515%2F9780691197777&publication_year=2019&author=Schneider%2CS) 
    
*   Schneider, S., & Turner, E. (2017). Is anyone home? A way to find out if AI has become self-aware. _Scientific American_. Retrieved from [https://blogs.scientificamerican.com/observations/is-anyone-home-a-way-to-find-out-if-ai-has-become-self-aware](https://blogs.scientificamerican.com/observations/is-anyone-home-a-way-to-find-out-if-ai-has-become-self-aware). Accessed Dec 2023
    
*   Singer, P. (2009). Reply. In J. A. Schaller (Ed.), _Peter singer under fire: The moral iconoclast faces his critics_ (pp. 97–102). Open Court.
    
    [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Reply&pages=97-102&publication_year=2009&author=Singer%2CP) 
    
*   Tegmark, M. (2017). _Life 3.0: Being human in the age of artificial intelligence_. Vintage.
    
    [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Life%203.0%3A%20Being%20human%20in%20the%20age%20of%20artificial%20intelligence&publication_year=2017&author=Tegmark%2CM) 
    
*   Turchin, A., & Denkenberger, D. (2020). Classification of global catastrophic risks connected with artificial intelligence. _Ai & Society,_ _35_(1), 147–163.
    
    [Article](https://link.springer.com/doi/10.1007/s00146-018-0845-5)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Classification%20of%20global%20catastrophic%20risks%20connected%20with%20artificial%20intelligence&journal=Ai%20%26%20Society&doi=10.1007%2Fs00146-018-0845-5&volume=35&issue=1&pages=147-163&publication_year=2020&author=Turchin%2CA&author=Denkenberger%2CD) 
    
*   Vilaça, M. M., & Lavazza, A. (2022). Not too risky. How to take a reasonable stance on human enhancement. _Filosofia Unisinos_, _23_(3), [https://doi.org/10.4013/fsu.2022.233.05](https://doi.org/10.4013/fsu.2022.233.05).
    
*   Yudkowsky, E. (2008). Artificial intelligence as a positive and negative factor in global risk. In M. M. Cirkovic & N. Bostrom (Eds.), _Global catastrophic risks._ Oxford University Press.
    
    [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Artificial%20intelligence%20as%20a%20positive%20and%20negative%20factor%20in%20global%20risk&publication_year=2008&author=Yudkowsky%2CE) 
