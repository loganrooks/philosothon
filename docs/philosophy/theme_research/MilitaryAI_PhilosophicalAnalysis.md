# **Military-Industrial Futurism: War, Capital, and the Shaping of AI Trajectories**

## **Introduction: Setting the Stage for Military-Industrial Futurism and AI**

The concept of "Military-Industrial Futurism" serves as a critical lens through which to examine the profound and often underestimated influence of military institutions, defense funding, and strategic imperatives on the trajectory of technological development, particularly concerning artificial intelligence (AI) and autonomous systems. This report undertakes a comprehensive philosophical analysis of this entanglement, exploring how the historical nexus of warfare, capitalism, and technological innovation continues to shape the present and future of AI. From the foundational role of agencies like the Defense Advanced Research Projects Agency (DARPA) in the genesis of computing and the internet to the contemporary development of autonomous weapons and predictive intelligence systems, military priorities have been deeply interwoven with the evolution of digital technologies.1

This analysis contends that military influence is not merely an external force acting upon AI development but is intrinsically linked to its conceptualization, architecture, and ultimate goals. This relationship potentially accelerates progress towards certain forms of transformative AI, such as Artificial General Intelligence (AGI), while simultaneously narrowing the scope of possibilities and foreclosing alternative developmental paths.3 Drawing upon key philosophical frameworks—including Martin Heidegger's critique of technology as "enframing" (*Gestell*), Jacques Ellul's analysis of "technique," Langdon Winner's concept of the "politics of artifacts," Paul Virilio's "dromology" and "war model," Michel Foucault's theories of power/knowledge and surveillance, and insights from Science and Technology Studies (STS)—this report interrogates how military imperatives may co-determine future technological possibilities, including potential singularity scenarios.5

The discussion is situated within contemporary societal anxieties regarding AI's impact, its dual-use nature encompassing both civilian and military applications, the ethical quandaries posed by lethal autonomous weapons systems (LAWS), and the speculative yet influential possibilities of AGI and technological singularity.3 The urgency of examining the military's role is underscored by the rapid pace of AI advancement and its potential to reshape global security, economic structures, and the very fabric of human society.10 This report will proceed by first establishing the philosophical foundations for critiquing technology and power, then tracing the historical role of the military in shaping the digital age. Subsequent sections will delve into critical theories illuminating the dynamics of speed, control, and knowledge; examine the interplay between technological potential and social choices; explore the implications for autonomous futures like LAWS and AGI; analyze the global geopolitical landscape and alternative technological visions; and finally, consider pathways toward more democratic governance of AI in light of pervasive military influence.

## **I. Philosophical Foundations: Deconstructing Technology and Power**

Understanding the deep entanglement of military imperatives and AI development requires foundational philosophical tools to deconstruct the nature of modern technology and its relationship with power. Thinkers like Martin Heidegger, Jacques Ellul, and Langdon Winner provide critical perspectives that move beyond simplistic instrumental views of technology, revealing how it shapes our perception, values, and political realities.

### **A. Heidegger's *Gestell*: Technology as Enframing and the Standing-Reserve**

Martin Heidegger's philosophy offers a profound critique of modern technology, arguing that its essence (*Wesen*) is not merely the sum of its tools or applications, but a specific way of revealing or understanding Being itself.12 He termed this essence *Gestell*, often translated as "enframing".14 *Gestell* represents a fundamental ontological framework that structures how entities—including nature, objects, and even human beings—appear to us in the modern technological age.14 Within this framework, everything is apprehended primarily as *Bestand*, typically translated as "standing-reserve" or "stock"—resources to be ordered, optimized, calculated, and made available for efficient use.18 This mode of revealing involves a "challenging-forth" (*Herausfordern*), where nature is compelled to yield energy or materials that can be extracted, stored, and distributed according to human designs, such as a hydroelectric dam transforming a river into a mere power source.5 Heidegger also associated this with the "gigantic," the domination of Being by machination, calculation, and quantification.20

This concept of *Gestell* is particularly relevant to understanding the military mindset and its influence on technology. Military logic inherently aligns with the principles of enframing, viewing the world—territory, natural resources, populations—as a collection of quantifiable and manageable resources to be controlled and exploited for strategic advantage.15 Human beings, whether combatants or civilians, risk being reduced to "human resources" within this framework, their value assessed based on their utility within a strategic calculation.15 The military's intrinsic drive for efficiency, control, and optimization resonates deeply with the logic of *Gestell*.19

Heidegger warned that *Gestell* constitutes the "supreme danger" because it threatens to become the *only* way of revealing, concealing all other possibilities of understanding Being and our relationship to the world.18 This includes the danger of humans themselves being fully absorbed into the standing-reserve, losing their unique capacity to question Being and thus undermining their autonomy.14 However, Heidegger also saw an ambiguity within this danger: precisely because *Gestell* is the dominant mode of revealing, recognizing it as such holds the potential for a "saving power" (*das Rettende*).5 This might involve cultivating a different relationship with technology, characterized by *Gelassenheit* ("releasement" or letting-be), a calm acceptance that allows us to use technology without being consumed by its underlying framework.5 It is important to note that Heidegger's critique of technology was intertwined with his controversial political views, including his critique of "Americanism" and Bolshevism as manifestations of technological frenzy and his temporary alignment with National Socialism, which he later framed, problematically, as an insufficient attempt to confront technology.18

Applied to artificial intelligence, especially in a military context, *Gestell* manifests as the reduction of complex phenomena—human intelligence, ethical decision-making, social interactions—into calculable, optimizable, and controllable processes.19 AI systems designed within this framework inherently prioritize efficiency, prediction, and control, often at the expense of other values like transparency, justice, or genuine understanding.22 The drive for military AI, therefore, is not just funded by military needs but is conceptually shaped by the enframing worldview.

This suggests that *Gestell* functions as a pre-conditioning framework for AGI. Heidegger's concept describes the ontological ground upon which modern technology arises.17 Military priorities, deeply resonant with the optimizing logic of *Gestell* 19, thus do more than simply direct funding; they shape the very understanding of what advanced AI *is* and *should be*. The problems deemed important for AGI to solve are framed in terms of control, prediction, and resource management—aligning with military needs. This inherently privileges AGI models that excel at these tasks, potentially rendering alternative conceptions of intelligence—perhaps more qualitative, contextual, or ethically attuned—as philosophically marginal or practically irrelevant within this dominant paradigm.3 In this way, the military-industrial embrace of *Gestell* shapes the future of AGI by constraining the conceptual space long before such systems might be realized.

### **B. Ellul's *Technique*: The Imperative of Efficiency and the Autonomous System**

Complementing Heidegger's ontological critique, French sociologist and philosopher Jacques Ellul offered a penetrating analysis of the societal impact of technology through his concept of *la technique*.24 Ellul defined *technique* not merely as machines or technology, but as "the totality of methods rationally arrived at and having absolute efficiency (for a given stage of development) in every field of human activity".26 It represents a pervasive logic, a way of thinking and organizing society that prioritizes efficiency, rationality, calculation, and optimization above all other values.26 Ellul argued that *technique* possesses several key characteristics: it seeks the "one best way" (rationality), it creates an artificial world, it tends towards automation, it grows and spreads relentlessly (self-augmentation), it integrates previously separate elements (monism), it aims to encompass all geographical and human domains (universalism), and crucially, it becomes autonomous, developing according to its own internal logic rather than human needs or desires.26

The military-industrial complex serves as a stark embodiment of *technique*.24 It is fundamentally geared towards achieving maximum efficiency in the development, production, and deployment of military capabilities.30 Military requirements, such as the need for better weapons or more efficient logistics, act as powerful stimuli for the advancement of *technique*.24 This relentless drive for efficiency often overshadows ethical considerations or human values.

Ellul warned that the dominance of *technique* leads to a society where the means (efficiency, technology) overwhelm the ends (human flourishing, freedom).26 *Technique* imposes a "technological imperative": whatever *can* be done technologically *will* be done, regardless of the consequences.28 This results in an autonomous technological system that escapes human control, shapes social structures to meet its own requirements, and ultimately diminishes human freedom by turning individuals into cogs within the system, serving the needs of the machine rather than the other way around.27 Education shifts towards STEM fields, military power becomes dependent on technological development, and even seemingly natural activities become co-opted by the logic of efficiency.26

AI development, particularly within the military sphere, clearly reflects the logic of *technique*. The drive for faster algorithms, more efficient data processing, automated targeting, and optimized logistics exemplifies this imperative.24 AI, as perhaps the ultimate expression of rational calculation and efficiency, threatens to accelerate the autonomy of the technological system, further removing human judgment and control from critical processes.27

While Heidegger's *Gestell* and Ellul's *Technique* offer distinct analytical angles—one focusing on the ontological mode of revealing, the other on the sociological drive for efficiency—they converge powerfully in the context of military AI. *Gestell* provides the philosophical worldview where everything is seen as a resource ripe for optimization.14 *Technique* supplies the relentless societal drive and the rational methodology to achieve that optimization in every domain.24 Military imperatives, demanding superiority through speed, efficiency, and control 19, act as the crucial catalyst and amplifier for this convergence. Military funding and requirements push AI development forward 1, ensuring it is not only *conceived* within the instrumental framework of *Gestell* but also relentlessly *pursued* through the optimizing methods of *Technique*. This synergy creates a potent feedback loop: military demands for efficient AI (*Technique*) are met by developing systems that view the world as resources (*Gestell*), which in turn reinforces the dominance of both *Technique* and *Gestell* within society. This dynamic helps explain the perceived inevitability 27 and rapid pace of military AI development, as well as the persistent concerns about its potential autonomy from meaningful human control.28

### **C. Winner's *Politics of Artifacts*: Embedding Military Values in AI Design**

Langdon Winner challenges the common notion that technologies are neutral tools, arguing instead that artifacts themselves can possess political qualities.33 In his seminal essay, "Do Artifacts Have Politics?", Winner presents two primary ways technology embodies politics.34 First, specific *technical arrangements* can function as forms of order, where design choices—whether intentional or unintentional—settle social issues or favor certain groups over others.33 Winner's famous, though debated, example is Robert Moses' low-hanging overpasses on Long Island parkways, allegedly designed to prevent buses carrying poorer citizens and minorities from accessing Jones Beach.33 Second, Winner argues that some technologies are *inherently political*, meaning their very nature requires or is strongly compatible with particular social and political structures.33 His example here is nuclear power, which he argues necessitates centralized, hierarchical, and potentially authoritarian control structures due to its inherent dangers and security requirements.33

Winner's framework provides a crucial lens for analyzing military AI systems. These systems are not merely neutral instruments but artifacts whose design choices can embed specific political and social values.38 For example:

* **AI Surveillance Systems:** The design of AI for surveillance often reflects a political prioritization of security over privacy. Furthermore, the datasets used to train these systems can embed existing societal biases (racial, gender, etc.), leading the AI artifact itself to perpetuate or even amplify discrimination.38  
* **Lethal Autonomous Weapons (LAWS):** The design of LAWS inherently involves political choices. Opting for full autonomy prioritizes military efficiency, speed, and force protection over human control, deliberation, and potentially the nuanced application of ethical principles like distinction and proportionality. Such designs embody specific stances on the value of human life and agency in warfare.38  
* **Command and Control (C2) Systems:** The architecture of AI-driven C2 systems can be designed to reinforce traditional military hierarchies or, alternatively, to enable more decentralized, networked forms of command. These architectural choices have direct political implications for power distribution and decision-making within the military organization.

Related concepts like Value Sensitive Design (VSD) attempt to proactively incorporate human values into technological design.35 However, in the military context, the dominant values being embedded, often implicitly, are those driven by strategic imperatives: efficiency, speed, control, lethality, and security.

Winner's analysis encourages a focus not just on the *use* of AI but on its underlying *architecture*. Military priorities like speed, security, hierarchical control, and network resilience 23 can become embedded in the fundamental design of AI algorithms, data structures, communication protocols, and hardware.23 For instance, an algorithm might be optimized for processing speed at the cost of explainability, or data access might be structured according to rigid security classifications reflecting military hierarchy. These architectural decisions, once solidified within the technology, carry political weight.38 They become part of the artifact itself, subtly shaping the possibilities and constraints for all interactions with that system, potentially favoring certain modes of operation (like centralized control or rapid, non-deliberative action) even if the technology is later adapted for civilian purposes. This makes the embedded military values less visible, more infrastructural, and consequently, more difficult to contest or change.

## **II. The Military Matrix: Forging the Digital Age**

The abstract philosophical critiques of technology find concrete expression in the history of computing, networking, and artificial intelligence. Military funding, research priorities, and operational requirements have not merely influenced these fields but have often been the primary catalysts, shaping their fundamental architectures and capabilities from the outset.

### **A. DARPA's Legacy: From ARPANET to AI – Funding and Architectural Influence**

The Defense Advanced Research Projects Agency (DARPA), established as ARPA in 1958 in direct response to the Soviet Union's launch of Sputnik, stands as a central institution in the story of military-driven technological innovation.1 Its mission was explicit: to prevent technological surprises and to fund high-risk, high-reward research and development (R\&D) that extended beyond the immediate needs of the individual military services.1 This unique mandate and structure allowed DARPA to play a foundational role in creating the technological landscape we inhabit today.

DARPA's influence on computing was profound and began early. It provided crucial funding for the development of time-sharing computer systems through initiatives like Project MAC at MIT, which led to the influential Multics operating system—a precursor to many modern OS concepts.1 The agency was instrumental in the early days of artificial intelligence research, supporting the work of pioneers like J.C.R. Licklider, who envisioned man-computer symbiosis, and funding projects like the development of Shakey the Robot at SRI.1 DARPA also funded Douglas Engelbart's groundbreaking work on hypertext and interactive computing, famously demonstrated in "The Mother of All Demos".1 The scale of DARPA's investment was significant, establishing and nurturing key university research centers at MIT, Stanford, and Carnegie Mellon, which became hubs for computer science and AI innovation.44

Perhaps DARPA's most widely recognized contribution is the creation of the ARPANET, the first large-scale packet-switching network and the direct forerunner of the modern Internet.1 Driven initially by the military need for resilient and survivable communication networks capable of withstanding partial outages (a concern heightened during the Cold War), ARPANET pioneered technologies and protocols, including TCP/IP, that form the bedrock of global digital communication.46

Throughout its history, DARPA has remained a major force in AI development. Following its early investments, it launched significant initiatives like the Strategic Computing Program in the 1980s, aimed at advancing AI, processing, and networking technologies.1 Today, it continues to invest heavily, with campaigns like "AI Next" committing billions of dollars to push the frontiers of AI, focusing on areas like contextual adaptation and machine common sense, moving beyond current machine learning paradigms.32 DARPA has funded work leading to breakthroughs in speech recognition, robotics, autonomous systems, and data analytics.44

The agency's unique funding model—empowering program managers, tolerating high levels of risk, and focusing on ambitious "grand challenges"—has been credited with enabling many of these breakthroughs.2 While the Mansfield Amendment in 1973 briefly restricted funding to projects with direct military application, potentially impacting the trajectory of research 1, DARPA's overall legacy demonstrates a consistent pattern of military investment seeding transformative technologies with vast civilian implications.

This foundational role, however, created a significant *path dependency*. The initial military requirements that guided DARPA's early investments—such as the need for network robustness, data security, integration with command and control structures, and processing speed 1—became embedded in the fundamental architectures and design philosophies of the resulting technologies.23 The protocols governing the internet, the structure of operating systems, and early approaches to AI all bear the imprint of these origins.46 Consequently, subsequent technological developments have often built upon these foundations, inheriting, sometimes implicitly, the design assumptions shaped by military logic. The internet's decentralized architecture, for example, while enabling global connectivity, also reflects the Cold War imperative for a network that could survive localized attacks. This path dependency means that the military logic of the past continues to shape the digital infrastructure of the present.

### **B. Command, Control, and Conflict: How Military Needs Shaped Computing and Networks**

Beyond DARPA's specific projects, the broader operational needs of military forces have consistently shaped the development trajectory of computing, networking, and AI. The relentless pursuit of effective Command and Control (C2) and superior Intelligence, Surveillance, and Reconnaissance (ISR) has been a primary driver of technological innovation within defense establishments.43

Historically, the military sought to harness computation to manage the increasing complexity of warfare and accelerate decision-making. Early efforts like the U.S. Army's Fieldata program and the CCIS-70 plan envisioned automated systems for various battlefield functions.23 The development of specific systems, such as the Tactical Fire Direction System (TACFIRE) for automating artillery calculations or the Tactical Operations System (TOS) for managing operational data, created concrete demands for specialized hardware, software, and data processing capabilities.23 Later concepts like the Army Tactical Command and Control System (ATCCS) aimed for greater integration across different battlefield functions (maneuver, fire support, air defense, intelligence, logistics), driving the need for interoperable systems and common communication protocols.23

These efforts were fueled by specific military requirements: the need to shorten the "sensor-to-shooter" timeline (detecting a target and engaging it rapidly) 50, achieve information superiority or "dominant battlespace knowledge" 23, enhance situational awareness for commanders, manage vast amounts of ISR data, and ensure seamless communication across different units and services (jointness).41 These operational priorities translated directly into technical requirements for increased computing power, higher network bandwidth, sophisticated data fusion algorithms, advanced sensors, AI for pattern recognition and analysis, and robust, secure communication architectures.23

The military's response involved developing and procuring technologies like automated fire control systems, integrated intelligence databases, secure data links, networked C2 platforms, precision-guided munitions, unmanned aerial vehicles (UAVs) equipped with advanced sensors, and increasingly, AI algorithms designed for tasks like target recognition, predictive analysis, and operational planning.23 This technological evolution, driven by military needs, also necessitated significant organizational adaptation within the armed forces, changing doctrines, training, and structures to accommodate and leverage the new capabilities.23

These historical trends continue today in concepts like Multi-Domain Operations (MDO) and Joint All-Domain Command and Control (JADC2).42 These doctrines envision highly integrated joint forces operating across land, sea, air, space, and cyberspace, relying heavily on advanced networking, AI-powered ISR fusion, rapid data sharing, and automated decision support systems to achieve dominance in high-tempo, contested environments.41

This long-term, systematic integration of computing, networking, and AI into core military functions points towards the emergence of what might be termed a "cyborg military." This is not merely about soldiers using advanced tools, but about the creation of a deeply intertwined socio-technical system where human cognitive processes, decision-making cycles, and operational tempos become inseparable from the automated and intelligent systems they rely upon.47 As military operations accelerate to match machine speeds 51, and as AI takes on more analytical and even advisory roles 41, human judgment risks being marginalized or overridden by the perceived efficiency and speed of the machine.54 This fusion of human and machine capabilities fundamentally alters the character of warfare, raising critical questions about human agency, the locus of control, and the nature of responsibility in future conflicts.

## **III. Critical Interrogations: Speed, Control, and Knowledge**

Beyond the historical tracing of military influence, critical theory offers powerful conceptual tools to interrogate the deeper philosophical implications of this entanglement. Thinkers like Paul Virilio, the Frankfurt School theorists, and Michel Foucault provide frameworks for analyzing how military preoccupations with speed, efficiency, control, and knowledge production shape technological trajectories, including AI, in ways that impact society far beyond the battlefield.

### **A. Virilio's Dromology: The War Model, Speed, and AI's Trajectory**

Paul Virilio, a French cultural theorist and urbanist profoundly influenced by his experiences during World War II, developed a unique critique centered on the concept of "dromology"—the logic and impact of speed.55 Virilio argued that speed, or velocity, is not merely a quantitative measure but a fundamental political and social force that shapes reality, perception, and power structures.55 He proposed a "war model" for understanding societal development, contending that technological innovation is primarily driven by military imperatives, particularly the pursuit of speed and logistical efficiency.55 For Virilio, the history of technology is inseparable from the history of warfare and its associated requirements for rapid movement, communication, and destruction. The state itself becomes "dromocratic," organized around the control and acceleration of movement.

This framework suggests that the military's inherent obsession with speed—faster reaction times, accelerated decision loops (like the OODA loop), rapid deployment, high-velocity weapons—directly shapes the trajectory of technological development, including AI.55 AI systems developed for military purposes are often optimized for speed in tasks like data processing, target identification, threat assessment, and autonomous navigation in dynamic environments.47 The demand for real-time performance in electronic warfare, cyber operations, and hypersonic missile defense further fuels the drive for AI capable of operating at superhuman speeds.

Virilio also presciently analyzed the shift towards information warfare and what he termed the "information bomb".55 In this new paradigm of conflict, the speed of information acquisition, processing, and dissemination becomes as critical as physical velocity. Control over the electromagnetic spectrum and cyberspace, and the ability to achieve "Global Information Dominance," become paramount military objectives.55 AI is central to this endeavor, enabling the rapid analysis of vast intelligence datasets, the development of automated cyber weapons, the crafting of disinformation, and the operation of pervasive surveillance systems.47

Applying Virilio's dromological lens to military AI reveals a potential danger: "dromological occlusion." The relentless focus on accelerating processes and shrinking timeframes, driven by military demands 41, risks obscuring or eliminating other crucial dimensions of reality. AI systems optimized solely for speed might achieve faster calculations or reactions but could lose the capacity for deep contextual understanding, nuanced ethical deliberation, or the recognition of subtle but critical environmental cues—all of which often require time for reflection and processing.54 In complex and unpredictable conflict scenarios, an AI operating at maximum velocity might make decisions that are technically rapid but contextually blind, ethically unsound, or strategically disastrous. This highlights a fundamental tension: the military imperative for speed may be inherently at odds with the requirements for safe, reliable, and ethically grounded AI, potentially creating systems that are fast but dangerously brittle.

### **B. Instrumental Reason and the Military-AI Complex: A Frankfurt School Critique**

The Frankfurt School of critical theory, particularly the work of Max Horkheimer, Theodor Adorno, and Herbert Marcuse, offers another powerful critique applicable to military AI through its analysis of "instrumental reason".58 Horkheimer, in *Critique of Instrumental Reason*, argued that Enlightenment reason, initially aimed at human emancipation, had become distorted in modern capitalist societies.60 Reason became primarily "instrumental" or "subjective," focused solely on calculating the most efficient means to achieve predetermined ends, regardless of the intrinsic value or ethical justification of those ends.58 This contrasts with "objective reason," which involves critical reflection on the ends themselves and alignment with universal truths or values.60

The Frankfurt School saw modern technology not as a neutral tool but as a primary embodiment of this instrumental reason, serving purposes of domination and control within industrial capitalism.58 Technology, driven by the logic of efficiency and quantification, could lead to new, more subtle forms of social control and the suppression of dissent.58 Marcuse, for instance, spoke of "technological rationality" creating a "one-dimensional society" where critical alternatives are absorbed or eliminated, and individuals become integrated into the existing system of production and consumption.58

The military-industrial complex represents a potent manifestation of instrumental reason.58 It operates on a logic of maximizing power, efficiency, and control through technological means. Complex geopolitical situations are often framed as technical problems requiring advanced technological solutions, and the development of ever more powerful and efficient weaponry becomes an end in itself, often detached from broader ethical debate or critical reflection on the nature of security.58

Military AI can be viewed as a potential apotheosis of instrumental reason applied to warfare.60 It promises the ultimate tools for calculation, prediction, optimization, and control, seemingly rationalizing the inherently chaotic and brutal nature of conflict. The development priorities often focus on quantifiable performance metrics—accuracy, speed, kill ratios, efficiency—potentially overshadowing qualitative concerns about ethical safeguards, human oversight, accountability, and the potential for unintended consequences.60

This application of instrumental reason via AI risks *reifying* conflict. By framing war and security challenges primarily as technical problems solvable through optimized algorithms, data analysis, and autonomous systems 47, the underlying political, social, economic, and human dimensions of conflict can be obscured. AI might make warfare seem more manageable, predictable, and "rational" from a purely technical standpoint, thereby potentially lowering the threshold for resorting to force 64 and masking the deep-seated structures of power and domination that often fuel conflict.58 The focus shifts from resolving underlying disputes to efficiently managing hostilities through technological means, a hallmark of instrumental reason's detachment from substantive goals and values.

### **C. Foucault's Lens: Surveillance, Power/Knowledge, and Algorithmic Governmentality**

Michel Foucault's analyses of power, knowledge, and surveillance provide indispensable tools for understanding the operation and implications of military AI. Foucault famously argued that power and knowledge are inextricably linked in a dynamic relationship he termed "power/knowledge" (*pouvoir/savoir*).65 Power is not simply repressive or held exclusively by the state; it is productive, relational, and circulates throughout the social body, operating at micro-levels.65 Power structures determine what counts as legitimate knowledge, while knowledge systems reinforce and extend the reach of power.65

Foucault identified distinct modalities of power. "Disciplinary power," exemplified by the Panopticon prison design, operates through constant surveillance (or the possibility thereof), normalization (establishing norms and identifying deviations), and examination, leading individuals to internalize control and regulate their own behavior.65 "Biopower," emerging later, focuses on managing populations as a whole—regulating life itself through statistics, public health measures, demographic analysis, and the administration of collective life.66 "Governmentality" refers to the broader "conduct of conduct," the diverse techniques and rationalities used to govern individuals and populations.69

Military AI systems, encompassing surveillance platforms, intelligence analysis tools, predictive algorithms, and autonomous systems, can be analyzed as key components of a Foucaultian *dispositif* or apparatus—a heterogeneous ensemble of discourses, institutions, architectural forms, regulations, and technological artifacts that support and maintain the exercise of power.66

AI-driven surveillance technologies directly echo Foucault's concept of panopticism. The potential for persistent monitoring by drones, satellites, cyber probes, and sensor networks, coupled with AI's ability to analyze the resulting data, creates a pervasive sense of visibility, potentially inducing self-discipline or altering behavior among targeted populations or even one's own forces.67 AI algorithms enact disciplinary power by establishing data-driven norms of "normal" or "suspicious" behavior, identifying anomalies, and flagging individuals or activities for scrutiny or intervention.69

Furthermore, military AI enables new forms of "algorithmic governmentality".69 By analyzing vast datasets, AI systems can categorize, predict, and manage populations in ways aligned with security objectives. This can involve predictive policing in counter-insurgency operations, identifying potential recruits for extremist groups based on online behavior, or even assessing the "neurobiological correlates" of radicalization to target interventions at the level of individual brains or "vulnerable populations".69 This intersection of AI, surveillance, and potentially neuroscience represents an extension of biopower, aiming to regulate not just behavior but potentially thoughts and emotions at a population level.69

The concept of power/knowledge highlights a critical feedback loop within military AI. Military power structures 41 define the objectives and select the data used to train AI systems, thereby shaping the *kind* of "knowledge" these systems produce (e.g., threat assessments, target identification).47 This AI-generated knowledge, often presented as objective and data-driven due to its technological origin, is then used to legitimize and direct the exercise of military power—authorizing strikes, intensifying surveillance, or implementing population control measures.54 This cycle can embed and amplify biases present in the initial data or objectives, making discriminatory or disproportionate outcomes appear as neutral, technical results, thus reinforcing existing power structures while expanding the reach and sophistication of military control.54

## **IV. Shaping the Path: Determinism, Construction, and Ethics**

The development of military AI is not unfolding in a vacuum. It is subject to intense debate regarding the forces driving its trajectory and the ethical considerations that should guide its path. Two dominant theoretical frameworks, technological determinism and social constructionism, offer contrasting views on the relationship between technology and society, while the inherent dual-use nature of AI and the challenges of opacity and emergence pose significant ethical and practical hurdles.

### **A. The Debate: Technological Determinism vs. Social Construction in Military AI**

The question of whether technology shapes society or society shapes technology lies at the heart of debates surrounding military AI. Technological determinism posits that technology develops according to its own internal logic, often driven by efficiency, and subsequently dictates social structures, cultural values, and historical development.71 A "hard" determinist view applied to military AI might suggest that the pursuit of autonomous weapons or an AI-driven transformation of warfare is an inevitable consequence of technological progress, a force that humans cannot ultimately control.71 Technology is seen as the "key governing force".71 A "soft" determinist view allows for some human agency in choosing how to use technology but maintains that the technology itself is the primary engine of change.71

In contrast, the Social Construction of Technology (SCOT) perspective, emerging from Science and Technology Studies (STS), argues that human actions, social contexts, political decisions, and cultural values are the primary forces shaping technological development.71 SCOT emphasizes the role of "relevant social groups" (e.g., military designers, policymakers, engineers, even opposing activists) who hold different interpretations of a technology ("interpretive flexibility") and negotiate its meaning and form.72 Technologies are not preordained; they exhibit "design flexibility," meaning multiple pathways are possible, and the final stabilized form ("closure") reflects the interpretations and power dynamics of dominant social groups.72 Applied to military AI, SCOT suggests that its development is not inevitable but is actively shaped by the choices, priorities, funding decisions, ethical debates, and strategic doctrines of military institutions, governments, corporations, and researchers.72

Both perspectives face criticism. Determinism is often accused of neglecting human agency and social context, while SCOT has been criticized for potentially overlooking the real constraints and consequences of technology itself, ignoring marginalized groups without a voice in the construction process, and sometimes avoiding necessary moral evaluation.72 A more nuanced understanding likely lies in recognizing the interplay between technological affordances and social shaping. As Marshall McLuhan suggested, "We shape our tools, and then our tools shape us".74 Technology opens possibilities and imposes constraints, but societal choices determine which possibilities are pursued and how constraints are navigated, especially in a domain as deliberately directed as military development.

Crucially, the debate itself is not merely academic; it is *performative* in the context of military AI. How key actors perceive the relationship between technology and society influences their actions.75 If military leaders and policymakers adopt a determinist stance, believing an AI arms race is inevitable and technological superiority is the only path 47, they are more likely to prioritize speed and investment over caution, diplomacy, or ethical constraints. This behavior, in turn, reinforces the perception of an inescapable technological race, potentially becoming a self-fulfilling prophecy.71 Conversely, embracing a social constructionist perspective highlights agency, responsibility, and the possibility of choice.72 This view encourages deliberate governance, ethical intervention, investment in safety research, and diplomatic efforts to shape AI's trajectory.77 Therefore, the philosophical framing adopted by powerful actors is itself a political act that actively influences the future it purports to describe.

### **B. The Dual-Use Conundrum: Navigating Civilian and Military AI Ethics**

Artificial intelligence exemplifies the challenge of dual-use technology—capabilities developed with both civilian and military applications.7 This inherent duality creates significant ethical dilemmas and governance challenges, particularly as the lines between civilian and military research and application become increasingly blurred.7

The core ethical problem lies in the potential for technologies developed for beneficial civilian purposes (e.g., medical diagnosis, image recognition for accessibility) to be repurposed for harmful military or surveillance applications (e.g., autonomous targeting, state surveillance).7 Conversely, military-funded research can lead to spin-offs benefiting society, but the technologies may carry embedded military logics or lack appropriate safeguards for civilian contexts.79 This ambiguity complicates regulation, as it is often difficult to categorize AI research or systems as purely civilian or military.7 Facial recognition algorithms, machine learning techniques for pattern analysis, and advances in robotics can serve vastly different ends depending on intent and deployment context.7

This blurring leads to several ethical quandaries:

* **Accountability:** Assigning responsibility becomes difficult when harm arises from a system whose origins or intended use is ambiguous, or when autonomous systems make decisions.7  
* **Arms Race Dynamics:** The fear that adversaries might weaponize seemingly civilian AI advancements can fuel a competitive "AI arms race," prioritizing speed over safety and ethical considerations.7  
* **Collaboration Dilemmas:** Intense debate surrounds the ethics of collaboration between civilian tech companies/researchers and military/intelligence agencies, as seen in controversies like Google's involvement in Project Maven.81 While some argue military funding drives crucial innovation 80, others fear complicity in unethical applications.  
* **Governance Gaps:** Traditional arms control and technology regulation frameworks struggle to address the intangible, rapidly evolving, and deeply integrated nature of AI.7 Factors like "distinguishability" (how easily military uses can be differentiated from civilian ones) and "integration" (how deeply embedded the technology is across sectors) significantly impact governance potential.79 For AI, distinguishability is generally low (software is opaque), and integration is high (AI is an enabling technology), making verification-based arms control extremely challenging.79

From a strategic perspective, the dual-use nature of AI may not be solely a problem but also a feature that facilitates military influence. The ambiguity allows military-funded research to proceed with potentially less public or ethical scrutiny if framed as general or civilian-applicable R\&D.32 It enables the rapid "spin-on" of commercial innovations for military purposes 79, leveraging the dynamism of the private sector. This inherent lack of clear boundaries makes tracking, regulating, and controlling the militarization of AI more difficult than with traditional, physically distinct weapon systems 7, allowing military interests to permeate the broader AI ecosystem more subtly and pervasively.

### **C. The Challenge of Opacity and Emergence in Military AI Systems**

Beyond dual-use issues, two intrinsic technical characteristics of advanced AI systems—opacity and emergent behavior—pose profound ethical and control challenges, especially in military contexts. Opacity, often referred to as the "black box" problem, describes the difficulty, sometimes impossibility, of understanding how complex AI systems, particularly those based on deep learning or other machine learning techniques, arrive at their outputs.54 Even the system's designers may not be able to fully trace the reasoning process or identify the specific factors driving a decision.83

Emergent behavior refers to novel and unpredictable system-level actions that arise from the complex interactions of individual components within a system.84 This is particularly relevant for multi-agent AI systems, such as drone swarms, where the collective behavior cannot be easily predicted from the programming of individual agents.84

These characteristics create significant problems for military applications 54:

* **Trust and Reliability:** Opacity makes it difficult for human commanders or operators to build justified trust in AI recommendations or actions. This can lead to either underutilization or, more dangerously, "automation bias"—an over-reliance on the system's output even when it might be flawed, simply because it is generated by a machine.54 Reliability itself becomes hard to assure if behavior can be emergent and unpredictable.84  
* **Accountability:** If an opaque or emergent AI system causes unintended harm or violates rules of engagement, establishing accountability becomes problematic. It is difficult to assign blame if the system's failure mode was unpredictable or its reasoning inscrutable.54  
* **Ethical Verification:** Ensuring compliance with International Humanitarian Law (IHL) principles like distinction and proportionality, or with ethical guidelines regarding bias, is challenging if the system's internal decision-making processes cannot be inspected or understood.54  
* **Meaningful Human Control:** Both opacity and emergence fundamentally challenge the possibility of maintaining meaningful human control. Control requires understanding and predictability, which these characteristics undermine.84  
* **Testing and Validation (T\&V):** Thoroughly testing and validating the safety and reliability of systems prone to opacity and emergence, especially across the vast range of potential real-world scenarios, is an immense technical hurdle.84

Efforts to mitigate these challenges include developing ethical AI principles that emphasize traceability, reliability, and governability 77, exploring technical solutions like Explainable AI (XAI) 82, maintaining humans "in" or "on" the loop (though this has its own complexities and limitations 85), rigorous T\&V protocols 85, and potentially embedding explicit ethical constraints into AI systems.85

However, the issues of opacity and emergence strike deeper than just rule compliance. They fundamentally challenge foundational military principles such as a clear chain of command, the ability to give lawful and understandable orders, the prediction of weapon effects, and the attribution of intent—all of which are prerequisites for traditional military doctrine and legal frameworks like IHL.8 The inability to fully understand, predict, or control AI actions introduces a radical uncertainty into the battlespace. This makes it difficult for commanders to exercise responsible command 8 and potentially undermines the very basis upon which military operations are planned, ethically justified, legally reviewed, and ultimately held accountable.84

## **V. Autonomous Futures: LAWS, AGI, and the Singularity Question**

The convergence of military imperatives and advancing AI capabilities points towards potentially transformative, and deeply controversial, futures. The development of Lethal Autonomous Weapon Systems (LAWS) raises immediate ethical and legal alarms, while the longer-term prospect of Artificial General Intelligence (AGI), potentially accelerated by military investment, evokes scenarios ranging from unprecedented progress to existential catastrophe.

### **A. Lethal Autonomous Weapons (LAWS): Ethics, Control, and the Responsibility Gap**

LAWS are defined as weapon systems capable of independently searching for, identifying, selecting, and engaging targets without direct human intervention in the critical decision loop.81 While semi-autonomous systems (e.g., defensive systems, "fire-and-forget" missiles) exist, the debate primarily concerns systems with autonomy in the final "select and engage" functions against targets, particularly human ones.89

Significant philosophical and ethical arguments have been marshaled against the development and deployment of LAWS 91:

* **The Responsibility Gap:** A central objection is that LAWS create an unacceptable gap in moral and legal responsibility.8 If an autonomous system commits an unlawful killing or causes unintended destruction, it is unclear who can be held accountable. Responsibility cannot be meaningfully transferred to a machine, which lacks moral agency and mortality.8 Attempts to distribute responsibility among programmers, manufacturers, and commanders face significant hurdles, especially given system complexity and unpredictability.8  
* **Meaningful Human Control (MHC):** LAWS inherently challenge, and potentially eliminate, the principle of MHC over the use of lethal force.8 Effective control requires understanding, predictability, and the ability to intervene, all of which are compromised by full autonomy, especially given the challenges of opacity and emergence.90 The proposed conditions for MHC—requiring systems to "track" relevant moral reasons and facts, and allowing actions to be "traced" back to human agents—are difficult to guarantee in fully autonomous systems.90  
* **Human Dignity:** Many argue that delegating the decision to kill a human being to a machine fundamentally violates human dignity.8 This argument suggests that the *process* by which life-or-death decisions are made matters intrinsically; reducing a human being to a data point processed by an algorithm for targeting is seen as inherently dehumanizing.89  
* **Compliance with International Humanitarian Law (IHL):** Serious doubts exist about whether LAWS can reliably comply with the core principles of IHL—distinction (discriminating between combatants and civilians/civilian objects), proportionality (ensuring expected collateral damage is not excessive relative to military advantage), and precaution (taking feasible steps to minimize civilian harm).8 These principles often require sophisticated, context-dependent human judgment, situational awareness, and potentially empathy, capabilities currently beyond AI.91  
* **Risk of Escalation and Accidents:** The speed and autonomy of LAWS could lead to rapid, unintended escalation of conflicts, making crisis management more difficult.92 Misidentification, algorithmic errors, or unforeseen interactions between autonomous systems could lead to catastrophic accidents.92  
* **Dehumanization of Warfare:** Removing human soldiers from the decision to use lethal force risks further abstracting and dehumanizing warfare, potentially lowering psychological and political barriers to conflict.92

Conversely, arguments are made in favor of pursuing LAWS development or, at least, against immediate bans 91:

* **Military Advantages:** LAWS promise significant tactical and strategic benefits, including operating at superhuman speeds, acting as force multipliers, undertaking dangerous missions without risking human lives, and overcoming human physiological or psychological limitations.93  
* **Potential for Greater Ethical Compliance:** Some proponents, like roboticist Ronald Arkin, argue that LAWS could potentially be *more* ethical than human soldiers.8 They would lack emotions like fear, anger, or revenge that can lead to war crimes; they could process more information objectively; potentially adhere more strictly to programmed rules of engagement; and might even report human ethical violations reliably.93  
* **Force Protection:** Militaries have a duty to protect their personnel, and deploying autonomous systems in dangerous situations aligns with this obligation.89  
* **Prematurity of Bans:** Critics of bans argue that the technology is still developing, its full capabilities and limitations are unknown, and foreclosing potential benefits (including potential ethical advantages) is premature.91  
* **Definitional and Enforcement Challenges:** Defining precisely what constitutes a "lethal autonomous weapon system" for a ban treaty is difficult, as is verifying compliance given the software-based nature of autonomy.93

Despite ongoing international discussions, particularly within the UN Convention on Certain Conventional Weapons (CCW), no international ban exists, and major military powers like the US, China, and Russia continue to invest heavily in autonomous capabilities.92

The push towards LAWS can be understood as the logical culmination of applying instrumental reason, *Technique*, and the *Gestell* framework to warfare. Within this paradigm 14, the human soldier—subject to fear, fatigue, emotion, bias, slow reaction times, and the need for self-preservation—can be viewed as an unpredictable and "inefficient" variable in the equation of lethal force.93 LAWS promise to remove this human element, optimizing warfare for speed, precision, and efficiency by reducing it to a calculated, automated process.93 The intense ethical debate surrounding LAWS 8 thus represents more than a disagreement over a specific technology; it signifies a fundamental clash over the desirability and ethical permissibility of this ultimate instrumentalization of killing, the endpoint of a technological trajectory deeply rooted in military logic and the philosophies of enframing and technique.

### **B. Military Catalysis: Accelerating AGI and Shaping Singularity Scenarios**

Beyond near-term concerns about LAWS lies the more speculative but potentially far more consequential prospect of Artificial General Intelligence (AGI)—AI possessing human-level cognitive abilities across a wide range of tasks—and the hypothetical Technological Singularity, a point of explosive, runaway technological growth potentially triggered by the emergence of Artificial Superintelligence (ASI).3 While AGI does not currently exist 3, its potential development is a subject of intense interest and investment, including within the military and national security communities.

Military establishments perceive potential strategic advantages in AGI.4 AGI could revolutionize intelligence analysis by processing unprecedented volumes of data, conduct complex strategic planning and war simulations, manage autonomous swarms, direct cyber warfare campaigns, or even provide a decisive edge in a future conflict.47 The belief that AI will fundamentally change warfare drives significant investment.47

This military interest acts as a powerful catalyst, potentially accelerating the timeline for AGI development through several mechanisms 4:

* **Massive Investment:** Governments, particularly defense departments, are pouring billions into AI research, providing resources that might not be available otherwise.32  
* **Goal-Oriented Research:** Military objectives offer concrete, complex problems (e.g., autonomous navigation in contested environments, real-time strategy adaptation) that push the boundaries of AI capabilities.  
* **Data Access:** Military and intelligence agencies possess vast, unique datasets (e.g., satellite imagery, signals intelligence) that could be leveraged for training sophisticated AI models.96  
* **Risk Tolerance:** National security imperatives might lead to a higher tolerance for risks associated with developing and testing advanced AI compared to the civilian sector, potentially speeding up experimentation and deployment.

However, military influence may not just accelerate the *arrival* of AGI but also shape its *nature* and the potential characteristics of a singularity.87 The emphasis is likely to be on developing AGI that is controllable, predictable, and aligned with national security objectives.94 Research might prioritize capabilities relevant to warfare: strategic analysis, deception, cyber operations, autonomous control of weapons platforms.47 Furthermore, the inherent secrecy and competitive nature of military R\&D ("AI arms race") could hinder global cooperation on AI safety research, increasing the risks associated with advanced AI development.4

This raises profound concerns about existential risk (x-risk) from AI.3 While conventional scenarios often focus on a "decisive" x-risk, such as an uncontrollable ASI rapidly taking over 6, military competition could also contribute to an "accumulative" x-risk pathway. This might involve the gradual deployment of increasingly complex, interconnected, yet potentially brittle AI systems within critical infrastructure and military C2 networks, eroding systemic resilience until a crisis triggers an irreversible collapse.6 Military pursuit of AGI, driven by competition, could exacerbate both types of risk.

A critical ethical issue arises concerning the very concept of "Friendly AI" or AI alignment within a military context.94 The goal of ensuring AGI/ASI is aligned with human values becomes complicated when development is driven by institutions whose primary values are national security, strategic advantage, and potentially prevailing in conflict.47 "Alignment" in this context might be interpreted primarily as alignment with the sponsoring state's military objectives and strategic interests, rather than with broader human values, cosmopolitan ethics, or global well-being.3 Consequently, military funding and direction could lead to the development of AGI systems that are technically "aligned" with their creators' military goals but are inherently competitive, potentially distrustful of other actors, and optimized for conflict. Such an AGI, even if not "rogue" in the classic sense, could pose a profound existential threat by dramatically increasing the likelihood and severity of great power conflict, rather than through unintended technical malfunction alone.6

## **VI. Global Dynamics and Alternative Visions**

The development and governance of military AI are not occurring in a uniform global context. Instead, they are deeply embedded within geopolitical rivalries and shaped by divergent national strategies, philosophies, and ethical frameworks. Simultaneously, alternative models for technological development exist, offering potential counter-narratives to the dominant military-driven approach.

### **A. Geostrategic AI: Comparing National Military Approaches and Philosophies**

The pursuit of AI capabilities, particularly for military applications, has become a central element of contemporary great power competition, most notably between the United States and China.9 This competition fuels perceptions of an "AI arms race," where nations feel compelled to accelerate development to avoid falling behind strategically.47 Different global actors, however, exhibit distinct approaches shaped by their political systems, strategic cultures, economic models, and ethical priorities.

* **United States:** The US aims to maintain its strategic position and technological edge by accelerating AI adoption while upholding ethical standards.47 Its strategy involves significant Department of Defense (DoD) investment (e.g., the $2 billion "AI Next" campaign), a focus on integrating AI across all warfighting domains (JADC2), and collaboration with allies.32 Philosophically, the US emphasizes democratic values, the Law of War, and human judgment.77 It has formalized this through the DoD AI Ethical Principles: Responsible, Equitable, Traceable, Reliable, and Governable.77 The US approach blends state-funded military R\&D with a strong reliance on its private technology sector, reflecting a more market-driven orientation compared to China.98  
* **China:** China's ambition is explicit: to become the world leader in AI by 2030, leveraging the technology for economic growth, social governance, and military modernization ("intelligentization") to achieve "world-class military" status.10 Key policies include the New Generation Artificial Intelligence Development Plan (AIDP) and the military-civil fusion (MCF) strategy, which aims to tightly integrate civilian and defense technological development.76 China's state-driven model 98 potentially offers advantages in data access (due to widespread surveillance and social credit systems) and rapid deployment, although the effectiveness of translating domestic surveillance AI into military capabilities is debated.10 While China has published ethical guidelines emphasizing fairness, transparency, and controllability, its approach is often perceived internationally as prioritizing state control and national power over individual rights and privacy.10  
* **European Union:** The EU's approach is characterized as "rights-driven" 98, prioritizing the protection of fundamental rights, democracy, and the rule of law.9 Its landmark AI Act aims to set a global standard for regulating AI based on risk levels, although it explicitly excludes systems developed solely for military purposes.9 The EU focuses on fostering "trustworthy AI" and maintaining a human-centric approach, generally exhibiting less direct emphasis on military AI development compared to the US and China.9  
* **Russia:** Russia recognizes AI's military potential and has invested in specific areas, particularly information warfare and potentially autonomous systems, aiming to disrupt adversaries.10 However, its overall capacity for large-scale AI development and implementation is considered less extensive than that of the US or China.10 Its approach to governance and ethics appears less formalized or publicly emphasized compared to the other major actors.

These divergent approaches create significant challenges for international governance and arms control.11 While there is growing recognition of AI risks and nascent dialogue initiatives (e.g., the Bletchley Declaration signed by both the US and China, the REAIM summits, the US-led Political Declaration on Responsible Military Use) 78, deep disagreements persist on fundamental principles, transparency levels, and the balance between innovation, security, and ethics.9

**Table 1: Comparative Analysis of National Military AI Strategies**

| Feature | United States | China | European Union | Russia (Inferred) |
| :---- | :---- | :---- | :---- | :---- |
| **Stated Goals** | Maintain strategic edge, ethical leadership, prevail in future conflicts 47 | Global AI leader by 2030, military modernization ("intelligentization"), CCP control 76 | Protect fundamental rights, foster innovation, trustworthy AI 9 | Military modernization, disruption, information warfare advantage 10 |
| **Key Strategic Policies** | DoD AI Strategy, AI Next campaign, JADC2, allied collaboration 32 | AIDP, Military-Civil Fusion (MCF), state investment, surveillance systems 76 | AI Act (excludes military), focus on high-risk civilian AI regulation 9 | State directives for military tech, focus on specific disruptive capabilities 10 |
| **Approach to Ethics/Gov.** | Formal DoD Ethical Principles, responsible AI focus, international engagement 77 | State-led guidelines, focus on security/control, emerging ethics codes 98 | Strong rights-based framework (AI Act), human-centric, ethical guidelines 9 | Less formalized public framework, likely state/security focused |
| **Underlying Philosophy** | Democratic values, Law of War, market elements, technological leadership 77 | State control, techno-nationalism, social stability (as defined by CCP) 10 | Human rights, rule of law, ethical principles, risk-aversion 98 | State power, strategic competition, asymmetric advantage 10 |
| **Key Investment Areas** | Broad AI R\&D, autonomous systems, ISR, C2, cyber 47 | Surveillance, autonomous systems, AI industry development, data infrastructure 10 | Primarily civilian AI R\&D, data protection, trustworthy AI systems 99 | Information warfare, potentially autonomous systems, specific military niches 10 |
| **Perceived Strengths** | Innovation ecosystem, allies, established ethical framework 97 | Data access, state direction, rapid adoption potential 10 | Strong regulatory/ethical framework, potential global standard-setter 9 | Niche disruptive capabilities, information operations expertise 10 |
| **Perceived Weaknesses** | Bureaucracy, potential ethical constraints slowing adoption 97 | Potential stifling of innovation, human rights concerns, international distrust 96 | Slower military AI development, fragmentation, AI Act exclusion 9 | Limited overall resources, less comprehensive AI ecosystem 10 |

The development and promotion of these national and regional ethical AI frameworks function not only as guides for responsible development but also as instruments of geopolitical influence. By establishing norms and standards reflecting their own values (e.g., US DoD principles, EU AI Act) 77, nations and blocs attempt to shape the global AI governance landscape.9 This serves to attract allies who share similar values, enhance interoperability based on common standards 97, and potentially disadvantage rivals whose approaches (e.g., prioritizing state control over individual rights) conflict with these emerging norms.10 Thus, the discourse around "responsible AI" is intertwined with soft power projection and strategic competition, as actors seek to align the future of AI with their specific geopolitical interests and value systems.78

### **B. Beyond Military Frameworks: Exploring Alternative Technology Development Paths**

The dominance of military priorities in shaping AI, as critiqued by thinkers like Heidegger, Ellul, and Winner, raises the question of alternative pathways for technological development. Several frameworks and movements offer contrasting values and processes:

* **Social Shaping of Technology (SST) / Social Construction of Technology (SCOT):** These STS approaches emphasize that technology is not predetermined but is shaped by societal choices, values, and negotiations among different groups.72 They highlight the potential for conscious, democratic guidance of technological trajectories rather than passive acceptance of military or market dictates.  
* **Appropriate Technology:** This movement advocates for technologies designed to be fitting for their specific social, economic, and environmental contexts.103 It often prioritizes decentralization, user control, low environmental impact, and meeting basic human needs, contrasting sharply with large-scale, centralized, resource-intensive military technologies. Lewis Mumford's distinction between "democratic" (small-scale, empowering) and "authoritarian" (large-scale, centralizing) technologies resonates here.28  
* **Commons-Based Peer Production (CBPP):** This model, exemplified by open-source software development, relies on open collaboration, shared knowledge resources (the "commons"), and community-based governance.102 It contrasts with the proprietary development, secrecy, and hierarchical control often characteristic of military and corporate AI projects. Related concepts include distributed manufacturing networks like Fab Labs, which aim to democratize production.102  
* **Value Sensitive Design (VSD) / Design for Values:** These approaches explicitly seek to integrate human values—such as privacy, fairness, autonomy, sustainability—into the design process from the outset, rather than treating them as afterthoughts or secondary constraints.35 This contrasts with military design processes that may prioritize performance, lethality, or security above other values.  
* **Technology Justice / Grassroots Innovation:** These movements focus on ensuring equitable access to technology, empowering marginalized communities to adapt and create technologies meeting their own needs, and challenging power imbalances in technological development and deployment.103

These alternative frameworks embody different values compared to typical military AI development: transparency over secrecy, collaboration over competition, decentralization over centralization, sustainability and human well-being over pure efficiency or strategic advantage, and democratic participation over top-down control. If AI development were guided by such principles, it might prioritize applications like environmental monitoring, collaborative scientific discovery, personalized education, enhancing community resilience, or tools for democratic deliberation, rather than autonomous weapons or mass surveillance.

However, while these alternatives offer valuable counterpoints, they remain largely marginal compared to the immense resources and institutional power driving military AI. The potential of frameworks like Appropriate Technology or CBPP to genuinely counter the dominant trajectory depends critically on challenging the underlying political economy—the funding mechanisms, the influence of the military-industrial complex, and the national security narratives—that currently privilege military-driven innovation.28 Without addressing these entrenched power structures, alternative approaches risk being confined to niche applications or even being co-opted by the dominant system, failing to achieve transformative change in the overall direction of technology. Their existence highlights political choice, but their realization requires political action.

## **VII. Governing the Future: Towards Democratic Control of AI**

Given the profound influence of military imperatives on AI's trajectory and the significant ethical and societal risks involved, establishing effective democratic governance is paramount. This requires drawing on philosophical insights, critically assessing existing policy efforts, and developing robust mechanisms for oversight and control.

### **A. Philosophical Pathways for Democratic Governance of Military-Influenced AI**

The philosophical perspectives examined throughout this report offer crucial guidance for constructing frameworks for democratic governance of AI, particularly in light of military influence:

* **Heidegger's** critique of *Gestell* underscores the need for critical awareness.18 Governance must recognize that technology is not neutral but embodies a powerful way of revealing that can obscure other values. Fostering a "free relation" to technology, perhaps through *Gelassenheit*, means consciously choosing how and when to employ technological rationality, rather than being swept away by it.5 This implies governance structures that encourage reflection and resist the totalizing impulse of enframing.  
* **Ellul's** analysis of *Technique* highlights the imperative to actively assert human values and control over the autonomous drive for efficiency.27 Democratic governance must involve conscious efforts to "desacralize" technology 31, question its supposed inevitability, and subordinate technical means to democratically chosen ends, resisting the tendency for *Technique* to dictate societal direction.25  
* **Winner's** concept of the "politics of artifacts" demands scrutiny of the values embedded in AI system design.33 Governance should involve public deliberation and participatory processes to shape technological choices, making the political dimensions of technical decisions explicit and contestable.35 It requires looking "inside the black box" not just technically, but politically.  
* **The Frankfurt School's** critique of instrumental reason calls for ongoing critical social theory to expose how technology can serve domination.58 Governance requires fostering a critical public sphere capable of questioning the ends pursued by the military-industrial complex and resisting the reduction of complex issues to mere technical problems.62  
* **Foucault's** work on power/knowledge emphasizes the need for mechanisms that make power visible and accountable.66 Governance of military AI requires robust transparency measures, independent oversight, and ways to challenge the narratives produced by power/knowledge loops, particularly concerning surveillance and control.  
* **Science and Technology Studies (STS)**, particularly SCOT, underscores the reality of social choice in technological development.72 This mandates governance structures that facilitate broad stakeholder engagement, allowing diverse voices and values to shape AI trajectories beyond military and corporate interests.74

Synthesizing these perspectives suggests that effective democratic governance of military-influenced AI should be grounded in: **critical awareness** of technology's non-neutrality and embedded politics; the **primacy of human values** (dignity, autonomy, justice) over unchecked efficiency; **transparency** in design, development, and deployment processes; **public deliberation** involving diverse stakeholders; robust **accountability mechanisms** for harms caused; and active **support for technological diversity** beyond military-defined paths.

### **B. Policy Considerations and Ethical Frameworks in Practice**

Current efforts to govern military AI reflect a growing awareness of the challenges, but remain fragmented and often insufficient. Several approaches are emerging:

* **National Principles and Regulations:** The US DoD has adopted five ethical principles (Responsible, Equitable, Traceable, Reliable, Governable) as a framework for development and deployment.77 The EU AI Act, while excluding purely military AI, aims to set global standards for civilian AI based on risk and fundamental rights.9 China has also issued national guidelines and ethical principles, though often viewed through a lens of state control.98 These national approaches show convergence on some high-level concepts (e.g., human-centricity, reliability) but diverge significantly in emphasis and implementation, reflecting different political values.78  
* **International Initiatives:** Efforts at the UN (including CCW discussions on LAWS), regional summits like REAIM, the US-led Political Declaration on Responsible Military Use of AI and Autonomy, and high-level statements like the Bletchley Declaration indicate a global conversation is underway.78 However, achieving binding international agreements faces significant hurdles due to geopolitical competition, differing national interests, and lack of consensus on fundamental issues like defining LAWS or MHC.9  
* **Multi-stakeholder Dialogue:** There is increasing recognition that effective governance requires input beyond states, involving industry, academia, civil society, and technical experts.101 Platforms for such dialogue are seen as crucial for building trust, sharing knowledge, and developing practical governance solutions.101

Key policy areas requiring concrete action include:

* **Transparency and Auditability:** Developing standards and mechanisms for documenting AI training data, design choices, and operational parameters to enable scrutiny and accountability.77  
* **Testing, Evaluation, Verification, and Validation (TEVV):** Establishing rigorous, independent processes to test AI systems for safety, reliability, robustness, bias, and compliance with legal/ethical rules throughout their lifecycle.11  
* **Meaningful Human Control (MHC):** Operationalizing MHC, especially for systems capable of using force, by defining clear requirements for human supervision, intervention capabilities, and understanding.54 This includes establishing clear lines of human responsibility.8  
* **Bias Detection and Mitigation:** Implementing deliberate processes to identify and minimize harmful biases in data and algorithms.54  
* **Data Governance:** Establishing clear rules for the ethical collection, processing, and use of data for training and operating military AI systems.  
* **Arms Control and Non-Proliferation:** Developing specific regimes to address the proliferation risks of LAWS and other destabilizing AI capabilities, acknowledging the challenges of dual-use technology.64  
* **Public Education and Engagement:** Fostering greater public literacy and informed debate about the implications of military AI.7

### **C. Recommendations for Responsible Innovation and Oversight**

Moving towards more effective democratic governance requires deliberate action:

1. **Strengthen Democratic Oversight:** National legislatures need dedicated committees or bodies with technical expertise to scrutinize military AI budgets, programs, and ethical reviews. Independent auditing and assessment capabilities are crucial.  
2. **Enhance Transparency (Balanced):** While acknowledging legitimate security concerns, governments should increase transparency regarding military AI strategies, ethical frameworks, testing procedures, and incidents involving AI systems. Redacting and declassifying relevant policies where possible can build public trust.78  
3. **Fund Independent Ethics and Safety Research:** Significant public funding should be allocated to independent academic and civil society research on AI ethics, safety, control mechanisms, bias auditing, and the long-term societal impacts of military AI, separate from direct military funding streams.  
4. **Pursue Focused International Cooperation:** Despite geopolitical tensions, diplomatic efforts should persist, focusing on concrete risk reduction measures. Priority areas include establishing norms against AI use in nuclear command and control, developing confidence-building measures regarding LAWS development, and creating shared frameworks for incident reporting or crisis communication involving AI systems.76  
5. **Support Technological Alternatives:** Policies should actively encourage and fund research and development based on alternative frameworks (VSD, Appropriate Technology, Commons-based Production) to foster technological diversity and provide non-military pathways for AI innovation.  
6. **Integrate Robust Ethics Education:** Comprehensive ethics training, incorporating the philosophical critiques and governance challenges discussed here, should be mandatory for AI researchers, developers, military operators, and policymakers involved in the AI lifecycle.

A fundamental challenge remains the **governance paradox**: the very national security concerns and competitive pressures that accelerate military AI development 47 simultaneously foster the secrecy and unilateralism that impede the transparency, cooperation, and open deliberation essential for effective democratic governance and robust ethical oversight.9 Overcoming this paradox requires innovative approaches that build trust and accountability *despite* these pressures. This might involve verifiable technical limitations on certain AI capabilities, independent oversight bodies with appropriate security clearances, focusing international agreements on observable behaviors rather than internal designs 79, or developing tiered transparency protocols that share different levels of information with different stakeholders.78 Addressing this paradox is central to navigating the future of military AI responsibly.

## **Conclusion: Synthesizing the Entanglement and Charting a Course Forward**

This analysis has traced the deep entanglement of military institutions, defense funding, and security imperatives with the development trajectory of artificial intelligence. Drawing on foundational philosophy of technology and critical theory, it has argued that this relationship extends beyond mere funding to shape the very conceptualization, architecture, and perceived purpose of AI. Heidegger's *Gestell*, Ellul's *Technique*, and the Frankfurt School's instrumental reason converge in the military context, driving AI towards efficiency, control, and optimization, often within a worldview that reduces reality to calculable resources. Winner's "politics of artifacts" reveals how these military values become embedded in the design of AI systems, while Foucault's work illuminates the power/knowledge dynamics inherent in AI-driven surveillance and algorithmic governmentality.

The historical role of DARPA and persistent military needs for advanced C2 and ISR capabilities have created path dependencies, embedding military logic into the infrastructure of the digital age. This influence raises profound ethical challenges, particularly concerning the dual-use nature of AI, the opacity and emergent behavior of complex systems, the potential for unacceptable responsibility gaps with LAWS, and the possibility that military priorities might accelerate the development of AGI while shaping it towards strategic competition rather than global well-being. The current geopolitical landscape, marked by US-China rivalry and divergent national approaches to AI governance, further complicates efforts to establish effective oversight.

The core thesis emerging from this analysis is that military influence acts as a powerful co-determining factor in AI's future, accelerating development along specific pathways—particularly those emphasizing control, prediction, and automation—while potentially narrowing the field of possibility and marginalizing alternative, more human-centric or ecologically attuned technological visions. The perceived inevitability of an AI arms race or the deployment of LAWS is not simply a technological outcome but is actively shaped by philosophical assumptions, institutional priorities, and political choices, often obscured by the rhetoric of technical necessity.

Navigating this complex terrain requires moving beyond simplistic techno-optimism or dystopian fear. It demands sustained critical engagement from philosophers, ethicists, policymakers, technologists, and the public. Recognizing the non-neutrality of technology, questioning the dominance of instrumental reason, scrutinizing the politics embedded in design, demanding transparency and accountability, and actively supporting diverse technological pathways are essential steps. The stakes are immense, encompassing the future character of warfare, the stability of international relations, the preservation of human autonomy and dignity, and potentially the long-term trajectory of intelligence itself. The future of AI is not yet written; charting a course toward a more just and beneficial future requires conscious, deliberate, and democratically grounded choices, informed by a critical understanding of the powerful military-industrial forces currently shaping its path.

#### **Works cited**

1. DARPA \- Wikipedia, accessed April 21, 2025, [https://en.wikipedia.org/wiki/DARPA](https://en.wikipedia.org/wiki/DARPA)  
2. How DARPA Technology Influences Industrial Advancements ..., accessed April 21, 2025, [https://keybotic.com/how-darpa-technology-influences-industrial-advancements/](https://keybotic.com/how-darpa-technology-influences-industrial-advancements/)  
3. Full article: The risks associated with Artificial General Intelligence: A systematic review, accessed April 21, 2025, [https://www.tandfonline.com/doi/full/10.1080/0952813X.2021.1964003](https://www.tandfonline.com/doi/full/10.1080/0952813X.2021.1964003)  
4. 2020 Survey of Artificial General Intelligence Projects for Ethics, Risk, and Policy, accessed April 21, 2025, [https://gcrinstitute.org/papers/055\_agi-2020.pdf](https://gcrinstitute.org/papers/055_agi-2020.pdf)  
5. "Alle Apparate abschalten." Conceiving Love and Technology with Heidegger And Kittler, accessed April 21, 2025, [https://www.on-culture.org/journal/issue-9/alle-apparate-abschalten/](https://www.on-culture.org/journal/issue-9/alle-apparate-abschalten/)  
6. Two types of AI existential risk: decisive and accumulative \- PhilPapers, accessed April 21, 2025, [https://philpapers.org/archive/KASTTO-9.pdf](https://philpapers.org/archive/KASTTO-9.pdf)  
7. Artificial Intelligence as a Dual-Use Technology: A Balancing Act for ..., accessed April 21, 2025, [https://www.aiddais.org/artificial-intelligence-as-a-dual-use-technology-a-balancing-act-for-society/](https://www.aiddais.org/artificial-intelligence-as-a-dual-use-technology-a-balancing-act-for-society/)  
8. www.davidpublisher.com, accessed April 21, 2025, [https://www.davidpublisher.com/Public/uploads/Contribute/5b9b751b8583b.pdf](https://www.davidpublisher.com/Public/uploads/Contribute/5b9b751b8583b.pdf)  
9. Governing Military AI Amid a Geopolitical Minefield | Carnegie Endowment for International Peace, accessed April 21, 2025, [https://carnegieendowment.org/research/2024/07/governing-military-ai-amid-a-geopolitical-minefield?lang=en¢er=europe](https://carnegieendowment.org/research/2024/07/governing-military-ai-amid-a-geopolitical-minefield?lang=en&center=europe)  
10. AI, China, Russia, and the Global Order: \- NSI, Inc., accessed April 21, 2025, [https://nsiteam.com/social/wp-content/uploads/2019/01/AI-China-Russia-Global-WP\_FINAL\_forcopying\_Edited-EDITED.pdf](https://nsiteam.com/social/wp-content/uploads/2019/01/AI-China-Russia-Global-WP_FINAL_forcopying_Edited-EDITED.pdf)  
11. Full article: AI Technologies and International Relations \- Taylor & Francis Online, accessed April 21, 2025, [https://www.tandfonline.com/doi/full/10.1080/03071847.2024.2392394](https://www.tandfonline.com/doi/full/10.1080/03071847.2024.2392394)  
12. The Technological View of the World of Martin Heidegger \- FutureLearn, accessed April 21, 2025, [https://www.futurelearn.com/info/courses/philosophy-of-technology/0/steps/26314](https://www.futurelearn.com/info/courses/philosophy-of-technology/0/steps/26314)  
13. Heidegger's Technologies, Postphenomenological Perspectives | Request PDF, accessed April 21, 2025, [https://www.researchgate.net/publication/287774504\_Heidegger's\_Technologies\_Postphenomenological\_Perspectives](https://www.researchgate.net/publication/287774504_Heidegger's_Technologies_Postphenomenological_Perspectives)  
14. Heidegger's Essentialist Responses to the Challenge of Technology | The Ted K Archive, accessed April 21, 2025, [https://www.thetedkarchive.com/library/david-edward-tabachnick-heidegger-s-essentialist-responses-to-the-challenge-of-technology](https://www.thetedkarchive.com/library/david-edward-tabachnick-heidegger-s-essentialist-responses-to-the-challenge-of-technology)  
15. Das Gestell and Human Autonomy: On Andrew Feenberg's ..., accessed April 21, 2025, [https://dc.etsu.edu/cgi/viewcontent.cgi?article=1270\&context=honors](https://dc.etsu.edu/cgi/viewcontent.cgi?article=1270&context=honors)  
16. On Heidegger's Nazism and Philosophy \- UC Press E-Books Collection, accessed April 21, 2025, [https://publishing.cdlib.org/ucpressebooks/view?docId=ft6q2nb3wh\&chunk.id=d0e6502](https://publishing.cdlib.org/ucpressebooks/view?docId=ft6q2nb3wh&chunk.id=d0e6502)  
17. “Enframing” Modernity: Heidegger, Technology, and the Human Condition, accessed April 21, 2025, [https://jamescungureanu.com/2024/12/30/enframing-modernity-heidegger-technology-and-the-human-condition/](https://jamescungureanu.com/2024/12/30/enframing-modernity-heidegger-technology-and-the-human-condition/)  
18. Understanding Heidegger on Technology \- The New Atlantis, accessed April 21, 2025, [https://www.thenewatlantis.com/publications/understanding-heidegger-on-technology](https://www.thenewatlantis.com/publications/understanding-heidegger-on-technology)  
19. Martin Heidegger (Stanford Encyclopedia of Philosophy), accessed April 21, 2025, [https://plato.stanford.edu/entries/heidegger/](https://plato.stanford.edu/entries/heidegger/)  
20. Catchwords: On Heidegger and “Americanism” \- PhilArchive, accessed April 21, 2025, [https://philarchive.org/archive/GIOCOH](https://philarchive.org/archive/GIOCOH)  
21. Applying Martin Heidegger's Enframing (Gestell) to AI \- TinyComputers.io, accessed April 21, 2025, [https://tinycomputers.io/posts/applying-martin-heideggers-enframing-gestell-to-ai.html](https://tinycomputers.io/posts/applying-martin-heideggers-enframing-gestell-to-ai.html)  
22. Iain D. Thomson: Heidegger on Technology's Danger and Promise in the Age of AI, accessed April 21, 2025, [https://reviews.ophen.org/2025/04/10/iain-d-thomson-heidegger-on-technologys-danger-and-promise-in-the-age-of-ai/](https://reviews.ophen.org/2025/04/10/iain-d-thomson-heidegger-on-technologys-danger-and-promise-in-the-age-of-ai/)  
23. apps.dtic.mil, accessed April 21, 2025, [https://apps.dtic.mil/sti/tr/pdf/ADA342549.pdf](https://apps.dtic.mil/sti/tr/pdf/ADA342549.pdf)  
24. Jacques Ellul | Philosophy of Technology, Technique, & Facts ..., accessed April 21, 2025, [https://www.britannica.com/biography/Jacques-Ellul](https://www.britannica.com/biography/Jacques-Ellul)  
25. The Word: Jacques Ellul's Dialogic Response To La Technique \- Duquesne Scholarship Collection, accessed April 21, 2025, [https://dsc.duq.edu/cgi/viewcontent.cgi?article=2458\&context=etd](https://dsc.duq.edu/cgi/viewcontent.cgi?article=2458&context=etd)  
26. The Technological Society by Jacques Ellul \- Goodreads, accessed April 21, 2025, [https://www.goodreads.com/book/show/274827.The\_Technological\_Society](https://www.goodreads.com/book/show/274827.The_Technological_Society)  
27. More than Machines? | Commonweal Magazine, accessed April 21, 2025, [https://www.commonwealmagazine.org/jacques-ellul-gertz-artificial-intelligence-AI-WGA-technology](https://www.commonwealmagazine.org/jacques-ellul-gertz-artificial-intelligence-AI-WGA-technology)  
28. The Interaction of Technology and Philosophy, accessed April 21, 2025, [https://appverse-technologies.com/2024/09/the-interaction-of-technology-and-philosophy/](https://appverse-technologies.com/2024/09/the-interaction-of-technology-and-philosophy/)  
29. Philosophy of Technology: An Introduction, accessed April 21, 2025, [https://shmu.ac.ir/file/download/page/1640673291-philosophy-of-technology-an-introduction.pdf](https://shmu.ac.ir/file/download/page/1640673291-philosophy-of-technology-an-introduction.pdf)  
30. Capital as Artificial Intelligence \- arXiv, accessed April 21, 2025, [https://arxiv.org/html/2407.16314v1](https://arxiv.org/html/2407.16314v1)  
31. Artificial Intelligence, or the End of Technics • Ill Will, accessed April 21, 2025, [https://illwill.com/end-of-technics](https://illwill.com/end-of-technics)  
32. DARPA's Impact on Artificial Intelligence \- AAAI Publications, accessed April 21, 2025, [https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/5294/7228](https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/5294/7228)  
33. Winner Do Artifacts Have Politics 1980 | PDF | Division Of Labour | Reason \- Scribd, accessed April 21, 2025, [https://www.scribd.com/document/203634721/Winner-Do-Artifacts-Have-Politics-1980](https://www.scribd.com/document/203634721/Winner-Do-Artifacts-Have-Politics-1980)  
34. faculty.cc.gatech.edu, accessed April 21, 2025, [https://faculty.cc.gatech.edu/\~beki/cs4001/Winner.pdf](https://faculty.cc.gatech.edu/~beki/cs4001/Winner.pdf)  
35. Architecture and Value-Sensitive Design | Request PDF \- ResearchGate, accessed April 21, 2025, [https://www.researchgate.net/publication/300053328\_Architecture\_and\_Value-Sensitive\_Design](https://www.researchgate.net/publication/300053328_Architecture_and_Value-Sensitive_Design)  
36. Do Politics Have Artefacts? \- Bernward Joerges, accessed April 21, 2025, [https://nissenbaum.tech.cornell.edu/papers/artefacts.pdf](https://nissenbaum.tech.cornell.edu/papers/artefacts.pdf)  
37. Do Politics have Artefacts \- EconStor, accessed April 21, 2025, [https://www.econstor.eu/bitstream/10419/71061/1/do\_politics%20have%20artefacts\_2.pdf](https://www.econstor.eu/bitstream/10419/71061/1/do_politics%20have%20artefacts_2.pdf)  
38. d101vc9winf8ln.cloudfront.net, accessed April 21, 2025, [https://d101vc9winf8ln.cloudfront.net/syllabuses/107330/original/Pruss\_PHIL\_694-005\_Fall2024.pdf?1726665871](https://d101vc9winf8ln.cloudfront.net/syllabuses/107330/original/Pruss_PHIL_694-005_Fall2024.pdf?1726665871)  
39. The Impact of AI on Future Education A Research Paper submitted to the Department of Engineering and Society Presented to the Fa \- LibraETD, accessed April 21, 2025, [https://libraetd.lib.virginia.edu/downloads/bc386k088?filename=2\_Clatterbuck\_Jacob\_Prospectus.pdf](https://libraetd.lib.virginia.edu/downloads/bc386k088?filename=2_Clatterbuck_Jacob_Prospectus.pdf)  
40. accessed December 31, 1969, [https://philarchive.org/citations/WINDAH-3?langFilter=off\&onlineOnly=\&direction=citations\&showCategories=off\&url=\&freeOnly=\&categorizerOn=off\&hideAbstracts=off\&page\_size=50\&proOnly=off\&sqc=off\&offset=50\&total=327\&newWindow=off\&filterByAreas=off\&eId=WINDAH-3](https://philarchive.org/citations/WINDAH-3?langFilter=off&onlineOnly&direction=citations&showCategories=off&url&freeOnly&categorizerOn=off&hideAbstracts=off&page_size=50&proOnly=off&sqc=off&offset=50&total=327&newWindow=off&filterByAreas=off&eId=WINDAH-3)  
41. The Tech Revolution and Irregular Warfare: Leveraging Commercial ..., accessed April 21, 2025, [https://www.csis.org/analysis/tech-revolution-and-irregular-warfare-leveraging-commercial-innovation-great-power](https://www.csis.org/analysis/tech-revolution-and-irregular-warfare-leveraging-commercial-innovation-great-power)  
42. A Symphony of Capabilities: How the Joint Warfighting Concept Guides Service Force Design and Development \- National Defense University Press, accessed April 21, 2025, [https://ndupress.ndu.edu/Media/News/News-Article-View/Article/3568312/a-symphony-of-capabilities-how-the-joint-warfighting-concept-guides-service-for/](https://ndupress.ndu.edu/Media/News/News-Article-View/Article/3568312/a-symphony-of-capabilities-how-the-joint-warfighting-concept-guides-service-for/)  
43. Intelligence, Surveillance, and Reconnaissance Design for Great Power Competition \- FAS Project on Government Secrecy, accessed April 21, 2025, [https://sgp.fas.org/crs/intel/R46389.pdf](https://sgp.fas.org/crs/intel/R46389.pdf)  
44. Knowledge Representation and Reasoning — A History DARPA Leadership \- AAAI Publications, accessed April 21, 2025, [https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/5295/7229](https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/5295/7229)  
45. There Was No 'First AI Winter' \- Communications of the ACM, accessed April 21, 2025, [https://cacm.acm.org/opinion/there-was-no-first-ai-winter/](https://cacm.acm.org/opinion/there-was-no-first-ai-winter/)  
46. A Brief History of the Internet & Related Networks, accessed April 21, 2025, [https://www.internetsociety.org/internet/history-internet/brief-history-internet-related-networks/](https://www.internetsociety.org/internet/history-internet/brief-history-internet-related-networks/)  
47. Digital Targeting: Artificial Intelligence, Data, and Military Intelligence \- Oxford Academic, accessed April 21, 2025, [https://academic.oup.com/jogss/article/9/2/ogae009/7667104](https://academic.oup.com/jogss/article/9/2/ogae009/7667104)  
48. Innovation Timeline | DARPA, accessed April 21, 2025, [https://www.darpa.mil/about/innovation-timeline](https://www.darpa.mil/about/innovation-timeline)  
49. (PDF) Military Applications of Artificial Intelligence: Ethical Concerns in an Uncertain World, accessed April 21, 2025, [https://www.researchgate.net/publication/340991921\_Military\_Applications\_of\_Artificial\_Intelligence\_Ethical\_Concerns\_in\_an\_Uncertain\_World](https://www.researchgate.net/publication/340991921_Military_Applications_of_Artificial_Intelligence_Ethical_Concerns_in_an_Uncertain_World)  
50. Military Transformation: Intelligence, Surveillance and Reconnaissance \- Every CRS Report, accessed April 21, 2025, [https://www.everycrsreport.com/reports/RL31425.html](https://www.everycrsreport.com/reports/RL31425.html)  
51. Army Futures Command Concept for Fires 2028, accessed April 21, 2025, [https://api.army.mil/e2/c/downloads/2021/10/06/869ca62b/afc-concept-for-fires-2028-oct21.pdf](https://api.army.mil/e2/c/downloads/2021/10/06/869ca62b/afc-concept-for-fires-2028-oct21.pdf)  
52. Doctrine, Technology, and War \- Air University, accessed April 21, 2025, [https://www.airuniversity.af.edu/Portals/10/ASPJ/journals/Chronicles/watts.pdf](https://www.airuniversity.af.edu/Portals/10/ASPJ/journals/Chronicles/watts.pdf)  
53. SourceS of Weapon SyStemS InnovatIon In the Department of DefenSe, accessed April 21, 2025, [https://history.defense.gov/Portals/70/Documents/acquisition\_pub/CMH\_51-2-1.pdf](https://history.defense.gov/Portals/70/Documents/acquisition_pub/CMH_51-2-1.pdf)  
54. Transcending weapon systems: the ethical challenges of AI in military decision support systems \- Blogs | International Committee of the Red Cross, accessed April 21, 2025, [https://blogs.icrc.org/law-and-policy/2024/09/24/transcending-weapon-systems-the-ethical-challenges-of-ai-in-military-decision-support-systems/](https://blogs.icrc.org/law-and-policy/2024/09/24/transcending-weapon-systems-the-ethical-challenges-of-ai-in-military-decision-support-systems/)  
55. (PDF) Ctheory Interview With Paul Virilio The Kosovo War Took ..., accessed April 21, 2025, [https://www.researchgate.net/publication/368387252\_Ctheory\_Interview\_With\_Paul\_Virilio\_The\_Kosovo\_War\_Took\_Place\_In\_Orbital\_Space](https://www.researchgate.net/publication/368387252_Ctheory_Interview_With_Paul_Virilio_The_Kosovo_War_Took_Place_In_Orbital_Space)  
56. Digital War: A Critical Introduction 9781138899865, 9781138899872, 9781315707624 \- DOKUMEN.PUB, accessed April 21, 2025, [https://dokumen.pub/digital-war-a-critical-introduction-9781138899865-9781138899872-9781315707624.html](https://dokumen.pub/digital-war-a-critical-introduction-9781138899865-9781138899872-9781315707624.html)  
57. The Photographic Universe: Vilém Flusser's Theories of Photography, Media, and Digital Culture \- CUNY Academic Works, accessed April 21, 2025, [https://academicworks.cuny.edu/context/gc\_etds/article/1707/viewcontent/THE\_PHOTOGRAPHIC\_UNIVERSE\_\_VILE%CC%81M\_FLUSSER\_S\_THEORIES\_OF\_PHOTOGRAPHY\_\_MEDIA\_\_AND\_DIGITAL\_CULTURE.pdf](https://academicworks.cuny.edu/context/gc_etds/article/1707/viewcontent/THE_PHOTOGRAPHIC_UNIVERSE__VILE%CC%81M_FLUSSER_S_THEORIES_OF_PHOTOGRAPHY__MEDIA__AND_DIGITAL_CULTURE.pdf)  
58. APPLIED SOCIAL THEORY: Frankfurt School and Critical Social ..., accessed April 21, 2025, [https://www.slideshare.net/slideshow/lecture-7-frankfurt-school-and-critical-social-theory/45881942](https://www.slideshare.net/slideshow/lecture-7-frankfurt-school-and-critical-social-theory/45881942)  
59. AI and the techno-utopian path not taken \- Le Monde diplomatique \- English edition, accessed April 21, 2025, [https://mondediplo.com/2024/08/07ai-cold-war](https://mondediplo.com/2024/08/07ai-cold-war)  
60. (PDF) Critique of Artificial Reason: Ontology of Human and Artificial ..., accessed April 21, 2025, [https://www.researchgate.net/publication/390727817\_Critique\_of\_Artificial\_Reason\_Ontology\_of\_Human\_and\_Artificial\_Intelligence](https://www.researchgate.net/publication/390727817_Critique_of_Artificial_Reason_Ontology_of_Human_and_Artificial_Intelligence)  
61. The Critical Humanism of the Frankfurt School as Social Critique (The Frankfurt School in New Times) 166694601X, 9781666946017 \- DOKUMEN.PUB, accessed April 21, 2025, [https://dokumen.pub/the-critical-humanism-of-the-frankfurt-school-as-social-critique-the-frankfurt-school-in-new-times-166694601x-9781666946017.html](https://dokumen.pub/the-critical-humanism-of-the-frankfurt-school-as-social-critique-the-frankfurt-school-in-new-times-166694601x-9781666946017.html)  
62. Review \- Justifying Ballistic Missile Defence \- e-IR.info, accessed April 21, 2025, [https://www.e-ir.info/2012/06/24/review-justifying-ballistic-missile-defence/](https://www.e-ir.info/2012/06/24/review-justifying-ballistic-missile-defence/)  
63. JOURNAL OF WORLD-SYSTEMS RESEARCH, accessed April 21, 2025, [https://jwsr.pitt.edu/ojs/jwsr/article/download/1332/1672/5743](https://jwsr.pitt.edu/ojs/jwsr/article/download/1332/1672/5743)  
64. The Ethics of Acquiring Disruptive Technologies: Artificial Intelligence, Autonomous Weapons, and Decision Support Systems \- National Defense University Press, accessed April 21, 2025, [https://ndupress.ndu.edu/Media/News/News-Article-View/Article/2054156/the-ethics-of-acquiring-disruptive-technologies-artificial-intelligence-autonom/](https://ndupress.ndu.edu/Media/News/News-Article-View/Article/2054156/the-ethics-of-acquiring-disruptive-technologies-artificial-intelligence-autonom/)  
65. Foucault's Theory of Power & Knowledge | Definition, Examples ..., accessed April 21, 2025, [https://www.perlego.com/knowledge/study-guides/foucaults-theory-of-power-knowledge/](https://www.perlego.com/knowledge/study-guides/foucaults-theory-of-power-knowledge/)  
66. Key concepts | Foucault News, accessed April 21, 2025, [https://michel-foucault.com/key-concepts/](https://michel-foucault.com/key-concepts/)  
67. Artificial Intelligence Law through the Lens of Michel Foucault: Biopower, Surveillance, and the Reconfiguration of Legal Normativity \- Scientific Research Publishing, accessed April 21, 2025, [https://www.scirp.org/journal/paperinformation?paperid=138133](https://www.scirp.org/journal/paperinformation?paperid=138133)  
68. Discipline and Power in the Digital Age: Critical Reflections from Foucault's Thought, accessed April 21, 2025, [https://rauli.cbs.dk/index.php/foucault-studies/article/download/7215/7489/23654](https://rauli.cbs.dk/index.php/foucault-studies/article/download/7215/7489/23654)  
69. uu.diva-portal.org, accessed April 21, 2025, [https://uu.diva-portal.org/smash/get/diva2:1857005/FULLTEXT01.pdf](https://uu.diva-portal.org/smash/get/diva2:1857005/FULLTEXT01.pdf)  
70. Michel Foucault: Political Thought \- Internet Encyclopedia of Philosophy, accessed April 21, 2025, [https://iep.utm.edu/fouc-pol/](https://iep.utm.edu/fouc-pol/)  
71. Technological determinism \- Wikipedia, accessed April 21, 2025, [https://en.wikipedia.org/wiki/Technological\_determinism](https://en.wikipedia.org/wiki/Technological_determinism)  
72. Social construction of technology \- Wikipedia, accessed April 21, 2025, [https://en.wikipedia.org/wiki/Social\_construction\_of\_technology](https://en.wikipedia.org/wiki/Social_construction_of_technology)  
73. Workshops and Conferences — Working Group on Philosophy of Technology \- KU Leuven, accessed April 21, 2025, [https://hiw.kuleuven.be/wgpt/conference](https://hiw.kuleuven.be/wgpt/conference)  
74. Technological Determinism vs. Social Construction of Technology \- YouTube, accessed April 21, 2025, [https://www.youtube.com/watch?v=Ogj-0nCV0L0](https://www.youtube.com/watch?v=Ogj-0nCV0L0)  
75. The Ethics of AI Innovation: Exploring Technological Determinism, Externalities, and Thought-Provoking Books, accessed April 21, 2025, [https://www.stauntonbooks.com/post/the-ethics-of-ai-innovation-exploring-technological-determinism-externalities-and-thought-provoki](https://www.stauntonbooks.com/post/the-ethics-of-ai-innovation-exploring-technological-determinism-externalities-and-thought-provoki)  
76. U.S.-China Competition and Military AI \- CNAS, accessed April 21, 2025, [https://www.cnas.org/publications/reports/u-s-china-competition-and-military-ai](https://www.cnas.org/publications/reports/u-s-china-competition-and-military-ai)  
77. DOD Adopts Ethical Principles for Artificial Intelligence \- Department of Defense, accessed April 21, 2025, [https://www.defense.gov/News/Releases/release/article/2091996/dod-adopts-ethical-principles-for-artificial-intelligence/](https://www.defense.gov/News/Releases/release/article/2091996/dod-adopts-ethical-principles-for-artificial-intelligence/)  
78. From Principles to Action: Charting a Path for Military AI Governance, accessed April 21, 2025, [https://www.carnegiecouncil.org/media/article/principles-action-military-ai-governance](https://www.carnegiecouncil.org/media/article/principles-action-military-ai-governance)  
79. Dual Use Deception: How Technology Shapes Cooperation in ..., accessed April 21, 2025, [https://www.cambridge.org/core/journals/international-organization/article/dual-use-deception-how-technology-shapes-cooperation-in-international-relations/C3BC65F4B54B509440632BD62D074031](https://www.cambridge.org/core/journals/international-organization/article/dual-use-deception-how-technology-shapes-cooperation-in-international-relations/C3BC65F4B54B509440632BD62D074031)  
80. AI as a dual-use technology – a cautionary tale \- Research Features, accessed April 21, 2025, [https://researchfeatures.com/ai-dual-use-technology-cautionary-tale/](https://researchfeatures.com/ai-dual-use-technology-cautionary-tale/)  
81. Taming the Killer Robot: Toward a Set of Ethical Principles for Military Artificial Intelligence \- Department of Defense, accessed April 21, 2025, [https://media.defense.gov/2022/Jul/13/2003034339/-1/-1/1/JIPA%20-%20GALLIOTT%20-%20JUL%2022.PDF](https://media.defense.gov/2022/Jul/13/2003034339/-1/-1/1/JIPA%20-%20GALLIOTT%20-%20JUL%2022.PDF)  
82. IEEE P7001: A Proposed Standard on Transparency \- Frontiers, accessed April 21, 2025, [https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2021.665729/full](https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2021.665729/full)  
83. Ethics of Artificial Intelligence and Robotics \- PhilArchive, accessed April 21, 2025, [https://philarchive.org/archive/MLLEOA-4v2](https://philarchive.org/archive/MLLEOA-4v2)  
84. Responsible AI Symposium – Implications of Emergent Behavior for Ethical AI Principles for Defense \- Lieber Institute, accessed April 21, 2025, [https://lieber.westpoint.edu/implications-emergent-behavior-ethical-artificial-intelligence-principles-defense/](https://lieber.westpoint.edu/implications-emergent-behavior-ethical-artificial-intelligence-principles-defense/)  
85. The comparative ethics of artificial-intelligence methods for military applications \- PMC, accessed April 21, 2025, [https://pmc.ncbi.nlm.nih.gov/articles/PMC9510613/](https://pmc.ncbi.nlm.nih.gov/articles/PMC9510613/)  
86. Responsible Use of AI in Military Systems \- OAPEN Library, accessed April 21, 2025, [https://library.oapen.org/bitstream/handle/20.500.12657/89843/9781040033739.pdf?sequence=1\&isAllowed=y](https://library.oapen.org/bitstream/handle/20.500.12657/89843/9781040033739.pdf?sequence=1&isAllowed=y)  
87. Full article: The AI Commander Problem: Ethical, Political, and Psychological Dilemmas of Human-Machine Interactions in AI-enabled Warfare \- Taylor & Francis Online, accessed April 21, 2025, [https://www.tandfonline.com/doi/full/10.1080/15027570.2023.2175887](https://www.tandfonline.com/doi/full/10.1080/15027570.2023.2175887)  
88. AI Principles: Recommendations on the Ethical Use of Artificial Intelligence by the Department of Defense Supporting Document D, accessed April 21, 2025, [https://media.defense.gov/2019/Oct/31/2002204459/-1/-1/0/DIB\_AI\_PRINCIPLES\_SUPPORTING\_DOCUMENT.PDF](https://media.defense.gov/2019/Oct/31/2002204459/-1/-1/0/DIB_AI_PRINCIPLES_SUPPORTING_DOCUMENT.PDF)  
89. Ethics and autonomous weapon systems: An ethical basis for human control? \- ICRC, accessed April 21, 2025, [https://www.icrc.org/en/download/file/69961/icrc\_ethics\_and\_autonomous\_weapon\_systems\_report\_3\_april\_2018.pdf](https://www.icrc.org/en/download/file/69961/icrc_ethics_and_autonomous_weapon_systems_report_3_april_2018.pdf)  
90. Meaningful Human Control over Autonomous Systems: A ..., accessed April 21, 2025, [https://pmc.ncbi.nlm.nih.gov/articles/PMC7806098/](https://pmc.ncbi.nlm.nih.gov/articles/PMC7806098/)  
91. The Debate Over Autonomous Weapons Systems, accessed April 21, 2025, [https://scholarlycommons.law.case.edu/cgi/viewcontent.cgi?params=/context/jil/article/1005/\&path\_info=47CaseWResIntlL2.Article.Noone\_26Noone.Print.pdf](https://scholarlycommons.law.case.edu/cgi/viewcontent.cgi?params=/context/jil/article/1005/&path_info=47CaseWResIntlL2.Article.Noone_26Noone.Print.pdf)  
92. States ought to ban lethal autonomous weapons. DEFINITIONS \- National Speech & Debate Association, accessed April 21, 2025, [https://www.speechanddebate.org/wp-content/uploads/JanFeb2021-LD-Topic-Analysis\_EB.pdf](https://www.speechanddebate.org/wp-content/uploads/JanFeb2021-LD-Topic-Analysis_EB.pdf)  
93. Pros and Cons of Autonomous Weapons Systems, accessed April 21, 2025, [https://www.armyupress.army.mil/Journals/Military-Review/English-Edition-Archives/May-June-2017/Pros-and-Cons-of-Autonomous-Weapons-Systems/](https://www.armyupress.army.mil/Journals/Military-Review/English-Edition-Archives/May-June-2017/Pros-and-Cons-of-Autonomous-Weapons-Systems/)  
94. Relying on the Kindness of Machines? The Security Threat of Artificial Agents, accessed April 21, 2025, [https://ndupress.ndu.edu/Media/News/News-Article-View/Article/581874/relying-on-the-kindness-of-machines-the-security-threat-of-artificial-agents/](https://ndupress.ndu.edu/Media/News/News-Article-View/Article/581874/relying-on-the-kindness-of-machines-the-security-threat-of-artificial-agents/)  
95. The “AI RMA” \- Andrew Marshall Foundation, accessed April 21, 2025, [https://www.andrewwmarshallfoundation.org/wp-content/uploads/2022/11/AIRMA\_FINAL.pdf](https://www.andrewwmarshallfoundation.org/wp-content/uploads/2022/11/AIRMA_FINAL.pdf)  
96. China's Pursuit of Defense Technologies: Implications for U.S. and Multilateral Export Control and Investment Screening Regimes \- CSIS, accessed April 21, 2025, [https://www.csis.org/analysis/chinas-pursuit-defense-technologies-implications-us-and-multilateral-export-control-and](https://www.csis.org/analysis/chinas-pursuit-defense-technologies-implications-us-and-multilateral-export-control-and)  
97. CSET \- Responsible and Ethical Military AI \- Center for Security and Emerging Technology, accessed April 21, 2025, [https://cset.georgetown.edu/wp-content/uploads/CSET-Responsible-and-Ethical-Military-AI.pdf](https://cset.georgetown.edu/wp-content/uploads/CSET-Responsible-and-Ethical-Military-AI.pdf)  
98. Envisioning a Global Regime Complex to Govern Artificial Intelligence, accessed April 21, 2025, [https://carnegieendowment.org/research/2024/03/envisioning-a-global-regime-complex-to-govern-artificial-intelligence?center=china](https://carnegieendowment.org/research/2024/03/envisioning-a-global-regime-complex-to-govern-artificial-intelligence?center=china)  
99. Governing artificial intelligence in China and the European Union: Comparing aims and promoting ethical outcomes \- Taylor & Francis Online, accessed April 21, 2025, [https://www.tandfonline.com/doi/full/10.1080/01972243.2022.2124565](https://www.tandfonline.com/doi/full/10.1080/01972243.2022.2124565)  
100. Promising Topics for US–China Dialogues on AI Safety and Governance, accessed April 21, 2025, [https://oms-www.files.svdcdn.com/production/downloads/academic/Final%20Promising%20Topics%20for%20US-China%20Dialogues%20on%20AI%20Governance%20and%20Safety.pdf?dm=1737452069](https://oms-www.files.svdcdn.com/production/downloads/academic/Final%20Promising%20Topics%20for%20US-China%20Dialogues%20on%20AI%20Governance%20and%20Safety.pdf?dm=1737452069)  
101. Governance of Artificial Intelligence in the Military Domain: A Multi-stakeholder Perspective on Priority Areas | UNIDIR, accessed April 21, 2025, [https://unidir.org/wp-content/uploads/2024/09/UNIDIR\_Governance\_of\_Artificial\_Intelligence\_in\_the\_Military\_Domain\_A\_Multi-stakeholder\_Perspective\_on\_Priority\_Areas.pdf](https://unidir.org/wp-content/uploads/2024/09/UNIDIR_Governance_of_Artificial_Intelligence_in_the_Military_Domain_A_Multi-stakeholder_Perspective_on_Priority_Areas.pdf)  
102. MAKING SUSTAINABILITY \- Aalto University Shop, accessed April 21, 2025, [https://shop.aalto.fi/media/attachments/f8dd3/Kohtala.pdf](https://shop.aalto.fi/media/attachments/f8dd3/Kohtala.pdf)  
103. Grassroots Innovation Movements \- Taylor & Francis eBooks, accessed April 21, 2025, [https://api.taylorfrancis.com/content/books/mono/download?identifierName=doi\&identifierValue=10.4324/9781315697888\&type=googlepdf](https://api.taylorfrancis.com/content/books/mono/download?identifierName=doi&identifierValue=10.4324/9781315697888&type=googlepdf)  
104. grassroots innovation movements \- OAPEN Library, accessed April 21, 2025, [https://library.oapen.org/bitstream/id/53f01cfb-95fe-4e94-8007-91b15975564c/9781317451198.pdf](https://library.oapen.org/bitstream/id/53f01cfb-95fe-4e94-8007-91b15975564c/9781317451198.pdf)